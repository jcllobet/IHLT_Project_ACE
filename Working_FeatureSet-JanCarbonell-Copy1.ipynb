{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Student: Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/jcllobet/general/b92a3192fbc943fa88114be7f92e916e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml in the top of the file for experiment tracking\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"WgXEAqBycAS6nrjJC5zkNTLA2\",\n",
    "                        project_name=\"general\", workspace=\"jcllobet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.778257Z",
     "start_time": "2018-12-11T01:01:53.997302Z"
    }
   },
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "#variable initialization and instantiation\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std_train = []\n",
    "gold_std_test = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.800801Z",
     "start_time": "2018-12-11T01:01:54.781241Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.816260Z",
     "start_time": "2018-12-11T01:01:54.802577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def preprocessing(line):\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    line = regex.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line)\n",
    "    line = regex.sub(r\"what's\", \"what is \", line)\n",
    "    line = regex.sub(r\"\\'s\", \" \", line)\n",
    "    line = regex.sub(r\"\\'ve\", \" have \", line)\n",
    "    line = regex.sub(r\"can't\", \"cannot \", line)\n",
    "    line = regex.sub(r\"n't\", \" not \", line)\n",
    "    line = regex.sub(r\"i'm\", \"i am \", line)\n",
    "    line = regex.sub(r\"\\'re\", \" are \", line)\n",
    "    line = regex.sub(r\"\\'d\", \" would \", line)\n",
    "    line = regex.sub(r\"\\'ll\", \" will \", line)\n",
    "    line = regex.sub(r\",\", \" \", line)\n",
    "    line = regex.sub(r\"\\.\", \" \", line)\n",
    "    line = regex.sub(r\"!\", \" ! \", line)\n",
    "    line = regex.sub(r\"\\/\", \" \", line)\n",
    "    line = regex.sub(r\"\\^\", \" ^ \", line)\n",
    "    line = regex.sub(r\"\\+\", \" + \", line)\n",
    "    line = regex.sub(r\"\\-\", \" - \", line)\n",
    "    line = regex.sub(r\"\\=\", \" = \", line)\n",
    "    line = regex.sub(r\"'\", \" \", line) #careful, it used to be \" \". Testing again for accuracy purposes. \n",
    "    line = regex.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", line)\n",
    "    line = regex.sub(r\":\", \" : \", line)\n",
    "    line = regex.sub(r\" e g \", \" eg \", line)\n",
    "    line = regex.sub(r\" b g \", \" bg \", line)\n",
    "    line = regex.sub(r\" u s \", \" american \", line)\n",
    "    line = regex.sub(r\"\\0s\", \"0\", line)\n",
    "    line = regex.sub(r\" 9 11 \", \"911\", line)\n",
    "    line = regex.sub(r\"e - mail\", \"email\", line)\n",
    "    line = regex.sub(r\"j k\", \"jk\", line)\n",
    "    line = regex.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    #only accept alphanum\n",
    "    # [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    #remove punctuation\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "#initializing stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#testing that it works\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.830135Z",
     "start_time": "2018-12-11T01:01:54.819191Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def stopwords_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "# convert words to tokens\n",
    "def pos_tag_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(pos_tag_to_lemmas, tokens))\n",
    "\n",
    "def pos_tag_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]\n",
    "\n",
    "def tokens_to_stemming(tokens):\n",
    "    return list(stemmer.stem(token) for token in tokens)\n",
    "    #print(tokens_to_stemms(['a','running','verbose','singing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synset, Nammed Entity and Content Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.020Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesking_sentence(pos_tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tagged_sentence]\n",
    "    result = []\n",
    "    \n",
    "    none_type_objects = []\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        # 'NoneType' object has no attribute 'name'\n",
    "        try:\n",
    "            result.append(lesk(sentence,word, wordnet_pos_code(tag)).name())\n",
    "        except:\n",
    "            result.append(word)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distances and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "\n",
    "#jaccard and edit distances\n",
    "def jaccard_d(list_0, list_1):\n",
    "    if len(set(list_0)) == 0 or len(set(list_1)) == 0:\n",
    "        return len(set(list_0).union(set(list_1)))\n",
    "    else:\n",
    "        return float(jaccard_distance(set(list_0),set(list_1)))\n",
    "    \n",
    "def edit_d(list_0, list_1):\n",
    "    return float(edit_distance(list_0,list_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.050Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_feature_generation(sent_0, sent_1):\n",
    "    #print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    stop_0, stop_1 = stopwords_from_sent(sent_0), stopwords_from_sent(sent_1)\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    stems_0, stems_1 = tokens_to_stemming(token_0), tokens_to_stemming(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    lesk_0, lesk_1 = lesking_sentence(pos_0), lesking_sentence(pos_1)\n",
    "    \n",
    "    \n",
    "\n",
    "    featureset = {\n",
    "        \"stops_jaccard\":jaccard_d(stop_0, stop_1),\n",
    "        \"stops_edit\":edit_d(stop_0, stop_1),\n",
    "        \"tokens_jaccard\":jaccard_d(token_0, token_1),\n",
    "        \"tokens_edit\":edit_d(token_0, token_1),\n",
    "        \"pos_jaccard\":jaccard_d(pos_0, pos_1),\n",
    "        \"pos_edit\":edit_d(pos_0, pos_1),\n",
    "        \"stems_jaccard\":jaccard_d(stems_0, stems_1),\n",
    "        \"stems_edit\":edit_d(stems_0, stems_1),\n",
    "        \"lemmas_jaccard\":jaccard_d(lemmas_0, lemmas_1),\n",
    "        \"lemmas_edit\":edit_d(lemmas_0, lemmas_1),\n",
    "        \"lesk_jaccard\":jaccard_d(lesk_0, lesk_1),\n",
    "        \"lesk_edit\":edit_d(lesk_0, lesk_1)\n",
    "    }\n",
    "    #\"NER\":[] --> DO ZALANDO's IMPLEMENTATION https://github.com/zalandoresearch/flair\n",
    "   \n",
    "    \n",
    "    return featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Finished Training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas_edit</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>lesk_edit</th>\n",
       "      <th>lesk_jaccard</th>\n",
       "      <th>pos_edit</th>\n",
       "      <th>pos_jaccard</th>\n",
       "      <th>stems_edit</th>\n",
       "      <th>stems_jaccard</th>\n",
       "      <th>stops_edit</th>\n",
       "      <th>stops_jaccard</th>\n",
       "      <th>tokens_edit</th>\n",
       "      <th>tokens_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lemmas_edit  lemmas_jaccard  lesk_edit  lesk_jaccard  pos_edit  \\\n",
       "0          7.0        0.526316        7.0      0.526316       7.0   \n",
       "1          5.0        0.500000        5.0      0.500000       5.0   \n",
       "2          7.0        0.642857        7.0      0.733333       7.0   \n",
       "3          6.0        0.388889        6.0      0.388889       6.0   \n",
       "4         14.0        0.888889       15.0      0.928571      16.0   \n",
       "\n",
       "   pos_jaccard  stems_edit  stems_jaccard  stops_edit  stops_jaccard  \\\n",
       "0     0.526316         7.0       0.526316         5.0       0.363636   \n",
       "1     0.500000         5.0       0.500000         3.0       0.800000   \n",
       "2     0.642857         7.0       0.642857         4.0       0.666667   \n",
       "3     0.388889         6.0       0.388889         3.0       0.333333   \n",
       "4     0.965517        14.0       0.888889         4.0       0.666667   \n",
       "\n",
       "   tokens_edit  tokens_jaccard  \n",
       "0          7.0        0.526316  \n",
       "1          5.0        0.500000  \n",
       "2          7.0        0.642857  \n",
       "3          6.0        0.388889  \n",
       "4         14.0        0.888889  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training data')\n",
    "X_train = [main_feature_generation(data[0], data[1]) for data in train_input]\n",
    "df_X_train = pd.DataFrame(X_train)\n",
    "training_scores_y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data\n",
      "Finished Testing!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas_edit</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>lesk_edit</th>\n",
       "      <th>lesk_jaccard</th>\n",
       "      <th>pos_edit</th>\n",
       "      <th>pos_jaccard</th>\n",
       "      <th>stems_edit</th>\n",
       "      <th>stems_jaccard</th>\n",
       "      <th>stops_edit</th>\n",
       "      <th>stops_jaccard</th>\n",
       "      <th>tokens_edit</th>\n",
       "      <th>tokens_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lemmas_edit  lemmas_jaccard  lesk_edit  lesk_jaccard  pos_edit  \\\n",
       "0          6.0        0.714286        8.0      0.941176       8.0   \n",
       "1          9.0        0.761905        9.0      0.761905      10.0   \n",
       "2         10.0        0.588235       10.0      0.500000      11.0   \n",
       "3          6.0        0.727273        6.0      0.727273       6.0   \n",
       "4         13.0        0.789474       13.0      0.789474      13.0   \n",
       "\n",
       "   pos_jaccard  stems_edit  stems_jaccard  stops_edit  stops_jaccard  \\\n",
       "0     0.875000         5.0       0.615385         5.0       0.625000   \n",
       "1     0.818182         9.0       0.761905         1.0       0.400000   \n",
       "2     0.666667        10.0       0.588235         2.0       0.400000   \n",
       "3     0.727273         5.0       0.600000         4.0       0.250000   \n",
       "4     0.789474        13.0       0.789474         4.0       0.666667   \n",
       "\n",
       "   tokens_edit  tokens_jaccard  \n",
       "0          7.0        0.800000  \n",
       "1         10.0        0.818182  \n",
       "2         10.0        0.588235  \n",
       "3          5.0        0.600000  \n",
       "4         13.0        0.789474  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing data')\n",
    "X_test = [main_feature_generation(data[0], data[1]) for data in test_input]\n",
    "df_X_test = pd.DataFrame(X_test)\n",
    "testing_scores_y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "df_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1854014 , -0.29878695,  0.06157077, ..., -0.11665755,\n",
       "         0.16169462, -0.40050658],\n",
       "       [-0.26212613, -0.4160811 , -0.34902353, ...,  0.22791104,\n",
       "        -0.27990169, -0.5245446 ],\n",
       "       [ 0.1854014 ,  0.22065857,  0.06157077, ...,  0.12262619,\n",
       "         0.16169462,  0.14880467],\n",
       "       ...,\n",
       "       [-1.38094494, -2.64466996, -1.37550928, ..., -0.40379804,\n",
       "        -1.38389245, -2.88126705],\n",
       "       [ 0.1854014 , -0.54717456,  0.26686791, ..., -0.0879435 ,\n",
       "         0.16169462, -0.66317533],\n",
       "       [-0.48588989,  1.0696448 , -0.55432068, ..., -0.40379804,\n",
       "        -0.50069984,  1.0466037 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_X_train)\n",
    "X_test_scaled = scaler.transform(df_X_test)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_changed = \"Run the main_featureset, added stopwords, removed the dots in .... printing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "MLP Training Accuracy:  0.848\n",
      "MLP Testing Accuracy:  0.709\n",
      "MLP Drop Train-Test:  0.139\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "itera = 1000\n",
    "r = MLPRegressor(max_iter=itera)\n",
    "r.fit(X_train_scaled, training_scores_y)\n",
    "r.score(X_train_scaled, training_scores_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(training_scores_y, train_prediction)[0]\n",
    "b = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('MLP Training Accuracy: ',round(a,3))\n",
    "print('MLP Testing Accuracy: ', round(b,3))\n",
    "print('MLP Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "#activation tanh or logistic give better performance\n",
    "solv = 'lbfgs'\n",
    "activ = 'logistic'\n",
    "r = MLPRegressor(max_iter=itera, solver=solv, hidden_layer_sizes=(100,), activation=activ)\n",
    "r.fit(X_train_scaled, training_scores_y)\n",
    "r.score(X_train_scaled, training_scores_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "c = pearsonr(training_scores_y, train_prediction)[0]\n",
    "d = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "gamma_type = 'scale'\n",
    "c_val = 1.0\n",
    "epsilon_val = 0.2\n",
    "r = SVR(gamma=gamma_type, C=c_val, epsilon=epsilon_val)\n",
    "r.fit(X_train_scaled, training_scores_y)\n",
    "r.score(X_train_scaled, training_scores_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "e = pearsonr(training_scores_y, train_prediction)[0]\n",
    "f = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "r = KNeighborsRegressor(n_neighbors=15)\n",
    "r.fit(X_train_scaled, training_scores_y)\n",
    "r.score(X_train_scaled, training_scores_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "g = pearsonr(training_scores_y, train_prediction)[0]\n",
    "h = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Training Accuracy: 0.848 w/mod_: 0.884 | SVR Training Accuracy: 0.836 | KNN Training Accuracy: 0.841\n",
      "MLP Testing Accuracy : 0.709 w/mod_: 0.662 | SVR Testing Accuracy : 0.709 | KNN Testing Accuracy : 0.692\n",
      "MLP Drop Train-Test  : 0.139 w/mod_: 0.222 | SVR Drop Train-Test :  0.127 | KNN Drop Train-Test  : 0.148\n"
     ]
    }
   ],
   "source": [
    "print('MLP Training Accuracy:',round(a,3), 'w/mod_:', round(c,3), '| SVR Training Accuracy:',round(e,3),'| KNN Training Accuracy:',round(g,3))\n",
    "print('MLP Testing Accuracy :', round(b,3), 'w/mod_:', round(d,3),'| SVR Testing Accuracy :', round(f,3), '| KNN Testing Accuracy :', round(h,3))\n",
    "print('MLP Drop Train-Test  :', round(a-b,3), 'w/mod_:', round(c-d,3),'| SVR Drop Train-Test : ', round(e-f,3), '| KNN Drop Train-Test  :', round(g-h,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be logged to your sklearn-demos project on Comet.ml\n",
    "params={\"main experiment changes\":what_changed,\n",
    "        \"random_state\":random_state,\n",
    "        \"MLP_iterations\": itera,\n",
    "        \"MLP_solver\":solv,\n",
    "        \"MLP_activation\":activ,\n",
    "        \"SVM_gamma\":gamma_type,\n",
    "        \"SVM_C\":c_val,\n",
    "        \"SVM_epsilon\":epsilon_val\n",
    "        #\"stratify\":True\n",
    "}\n",
    "\n",
    "metrics = {'MLP Training Accuracy':a,\n",
    "'MLP Testing Accuracy':b,\n",
    "'MLP Drop Train-Test':a-b,\n",
    "'MLP_mod Training Accuracy':c,\n",
    "'MLP_mod Testing Accuracy':d,\n",
    "'MLP_mod Drop Train-Test':c-d,\n",
    "'SVM Training Accuracy':e,\n",
    "'SVM Testing Accuracy':f,\n",
    "'SVM Drop Train-Test':e-f,\n",
    "'KNN Training Accuracy':g,\n",
    "'KNN Testing Accuracy':h,\n",
    "'KNN Drop Train-Test':g-h\n",
    "}\n",
    "\n",
    "experiment.log_dataset_hash(X_train_scaled)\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
