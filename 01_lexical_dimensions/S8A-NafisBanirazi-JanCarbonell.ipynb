{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 8\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab is to:\n",
    "-Consider the following sentence: \"Lazy cats play with mice.\"\n",
    "-Expand the grammar of the example related to non-probabilistic chart parsers in order to subsume this new sentence.\n",
    "-Perform the constituency parsing using a BottomUpChartParser, a BottomUpLeftCornerChartParser and a LeftCornerChartParser.\n",
    "-For each one of them, provide the resulting tree, the number of edges and the list of explored edges.\n",
    "-Which parser is the most efficient for parsing the sentence?\n",
    "-Which edges are filtered out by each parser and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constituency parsing with NLTK\n",
    "This section will define a grammar and use different parsing methods to determine the constituency tree of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Grammar definition\n",
    "The following cell defines a grammar that will allow to parse sentences with very specific terms and relations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "\n",
    "'''the grammar is expanded considering that:\n",
    "    - sentences are composed by noun and verb prhases\n",
    "    - \"mice\" is added as a possible plural name (NNS)\n",
    "    - \"with\" is added as a possible preposition (CC)\n",
    "    - \"play\" is added as the only possible verb (V)\n",
    "    - the verb phrase (VP) is defined as a verb (V) or a verb plus a prepositional phrase (V PP)\n",
    "    - the prepositional phrase is defined as a preposition plus a noun phrase (CC NP)\n",
    "'''\n",
    "grammar = CFG.fromstring('''\n",
    "                        S -> NP VP\n",
    "                        NP -> NNS | JJ NNS | NP CC NP \n",
    "                        NNS -> \"cats\" | \"dogs\" | \"mice\" | NNS CC NNS\n",
    "                        JJ -> \"big\" | \"small\" | \"lazy\"\n",
    "                        CC -> \"and\" | \"or\" | \"with\"\n",
    "                        V -> \"play\"\n",
    "                        VP -> V PP | V\n",
    "                        PP -> CC NP\n",
    "                        ''')\n",
    "\n",
    "# sentence tokens\n",
    "sentence = nltk.word_tokenize(\"Lazy cats play with mice\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 18 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> NNS\n",
      "    NP -> JJ NNS\n",
      "    NP -> NP CC NP\n",
      "    NNS -> 'cats'\n",
      "    NNS -> 'dogs'\n",
      "    NNS -> 'mice'\n",
      "    NNS -> NNS CC NNS\n",
      "    JJ -> 'big'\n",
      "    JJ -> 'small'\n",
      "    JJ -> 'lazy'\n",
      "    CC -> 'and'\n",
      "    CC -> 'or'\n",
      "    CC -> 'with'\n",
      "    V -> 'play'\n",
      "    VP -> V PP\n",
      "    VP -> V\n",
      "    PP -> CC NP\n",
      "['lazy', 'cats', 'play', 'with', 'mice']\n"
     ]
    }
   ],
   "source": [
    "print(grammar)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Constituency parsing\n",
    "The following cells perform the constituency parsing using a BottomUpChartParser, a BottomUpLeftCornerChartParser and a LeftCornerChartParser.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence to parse: ['lazy', 'cats', 'play', 'with', 'mice']\n",
      "\n",
      "Parsing with: BottomUpChartParser\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "Number of edges: 50\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:0] NP -> * JJ NNS\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:3] V  -> 'play' *\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[1:3] S  -> NP VP *\n",
      "[0:3] S  -> NP VP *\n",
      "[3:3] CC -> * 'with'\n",
      "[3:4] CC -> 'with' *\n",
      "[3:3] PP -> * CC NP\n",
      "[3:4] PP -> CC * NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "[3:5] PP -> CC NP *\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "[2:5] VP -> V PP *\n",
      "[1:5] S  -> NP VP *\n",
      "[0:5] S  -> NP VP *\n",
      "_________________________________________\n",
      "\n",
      "Parsing with: BottomUpLeftCornerChartParser\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "Number of edges: 31\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[1:2] S  -> NP * VP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] CC -> 'with' *\n",
      "[3:4] PP -> CC * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "[3:5] PP -> CC NP *\n",
      "[2:5] VP -> V PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "\n",
      "\n",
      "Edges that were filtered out:\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:0] NP -> * JJ NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[3:3] CC -> * 'with'\n",
      "[3:3] PP -> * CC NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "_________________________________________\n",
      "\n",
      "Parsing with: LeftCornerChartParser\n",
      "Number of trees:  1\n",
      "Parsed tree:  (S\n",
      "  (NP (JJ lazy) (NNS cats))\n",
      "  (VP (V play) (PP (CC with) (NP (NNS mice)))))\n",
      "Number of edges: 25\n",
      "[0:1] 'lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[1:2] S  -> NP * VP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V * PP\n",
      "[2:3] VP -> V *\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] CC -> 'with' *\n",
      "[3:4] PP -> CC * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[3:5] PP -> CC NP *\n",
      "[2:5] VP -> V PP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n",
      "\n",
      "\n",
      "Edges that were filtered out:\n",
      "[0:0] JJ -> * 'lazy'\n",
      "[0:0] NP -> * JJ NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:1] NP -> * NNS\n",
      "[1:1] NNS -> * NNS CC NNS\n",
      "[1:2] NNS -> NNS * CC NNS\n",
      "[1:1] S  -> * NP VP\n",
      "[1:1] NP -> * NP CC NP\n",
      "[1:2] NP -> NP * CC NP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:0] NP -> * NP CC NP\n",
      "[0:2] NP -> NP * CC NP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:2] VP -> * V PP\n",
      "[2:2] VP -> * V\n",
      "[3:3] CC -> * 'with'\n",
      "[3:3] PP -> * CC NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:4] NP -> * NNS\n",
      "[4:4] NNS -> * NNS CC NNS\n",
      "[4:5] NNS -> NNS * CC NNS\n",
      "[4:4] S  -> * NP VP\n",
      "[4:4] NP -> * NP CC NP\n",
      "[4:5] S  -> NP * VP\n",
      "[4:5] NP -> NP * CC NP\n",
      "_________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import BottomUpChartParser, ChartParser, LeftCornerChartParser\n",
    "\n",
    "total_edges = []\n",
    "\n",
    "print(\"Sentence to parse: {}\\n\".format(sentence))\n",
    "\n",
    "# using different parser classes\n",
    "parser_type = [(BottomUpChartParser, \"BottomUpChartParser\"), (ChartParser, \"BottomUpLeftCornerChartParser\"), (LeftCornerChartParser, \"LeftCornerChartParser\")]\n",
    "\n",
    "# using the grammar to create a parser, then parse the sentence with it \n",
    "for parser_class, parser_name in  parser_type:    \n",
    "    parser = parser_class(grammar)\n",
    "    parsed_sentence = parser.parse(sentence)\n",
    "\n",
    "    print(\"Parsing with: {}\".format(parser_name))\n",
    "    \n",
    "    # showing the constituency trees\n",
    "    possible_trees = []\n",
    "    for tree in parsed_sentence:\n",
    "        possible_trees.append(tree)\n",
    "    print(\"Number of trees: \", len(possible_trees))\n",
    "    for tree in possible_trees:\n",
    "        print(\"Parsed tree: \", tree)\n",
    "        \n",
    "    # list of the applied edges\n",
    "    parse = parser.chart_parse(sentence)\n",
    "    print(\"Number of edges: {}\".format(parse.num_edges()))\n",
    "    \n",
    "    edges = parse.edges()\n",
    "    for edge in edges:\n",
    "        print(edge)\n",
    "    if parser_class == BottomUpChartParser:\n",
    "        total_edges = edges\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(\"Edges that were filtered out:\")\n",
    "        for edge in total_edges:\n",
    "            if edge not in edges:\n",
    "                print(edge)\n",
    "    print(\"_________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "- (A) Which parser is the most efficient for parsing the sentence?\n",
    "- (B) Which edges are filtered out by each parser and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dependency parsing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('jumps', 'VBZ'), 'nsubj', ('Smith', 'NNP')), (('jumps', 'VBZ'), 'nmod', ('dog', 'NN')), (('dog', 'NN'), 'case', ('over', 'IN')), (('dog', 'NN'), 'det', ('the', 'DT')), (('dog', 'NN'), 'amod', ('lazy', 'JJ'))]\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "def get_dependency_triples(sentence):\n",
    "    \n",
    "    \"\"\"Returns an array with the triples of depencency parsing for the passed sentence\"\"\"\n",
    "    \n",
    "    # Core Named-entity parser as stanford one is deprecated: https://github.com/nltk/nltk/issues/2010\n",
    "    parser = CoreNLPDependencyParser(\"http://localhost:9000\")\n",
    "    parse = parser.raw_parse(sentence)\n",
    "    \n",
    "    # extract the triples from the depencency tree\n",
    "    triples = []\n",
    "    tree = next(parse)\n",
    "    for triple in tree.triples():\n",
    "        triples.append(triple)\n",
    "    return triples\n",
    "    \n",
    "print(get_dependency_triples(\"Smith jumps over the lazy dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: \t The bird is bathing in the sink. \n",
      "Second sentence: \t Birdie is washing itself in the water basin.\n",
      " \n",
      "\n",
      "First sentence: \t In May 2010, the troops attempted to invade Kabul. \n",
      "Second sentence: \t The US army invaded Kabul on May 7th last year, 2010.\n",
      " \n",
      "\n",
      "First sentence: \t John said he is considered a witness but not a suspect. \n",
      "Second sentence: \t \"He is not a suspect anymore.\" John said.\n",
      " \n",
      "\n",
      "First sentence: \t They flew out of the nest in groups. \n",
      "Second sentence: \t They flew into the nest together.\n",
      " \n",
      "\n",
      "First sentence: \t The woman is playing the violin. \n",
      "Second sentence: \t The young lady enjoys listening to the guitar.\n",
      " \n",
      "\n",
      "First sentence: \t John went horse back riding at dawn with a whole group of friends. \n",
      "Second sentence: \t Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#trial is on lab 08 folder:\n",
    "absolute_file_path = os.path.dirname(os.path.abspath(\"__file__\")) + \"/./trial//STS.input.txt\" \n",
    "\n",
    "\n",
    "#value initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "standard = []\n",
    "\n",
    "# find all sentence pairs in the document\n",
    "sentence_pairs = []\n",
    "sentence_set_pairs = []\n",
    "with open(absolute_file_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        index, sentence0, sentence1 = line.split(\"\\t\")\n",
    "        if index in d:\n",
    "            d[index] = sentence0, sentence1\n",
    "        else:\n",
    "            d[index] = (get_dependency_triples(sentence0), get_dependency_triples(sentence1))\n",
    "            print(\"First sentence: \\t\", sentence0, \"\\nSecond sentence: \\t\", sentence1, \"\\n\")\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence similarity calculation using triples vs the gold standard\n",
    "The pairs of sentences are checked to see how similar they are, using the Jaccard distance. Previously the sentences must have been tokenized and we have picked the **Sets**, *unique values of those tokenized sentences*; The more words or named entities two sentences have in common, the more similar they are. Then we calculate the similarity as 1-JD. \n",
    "\n",
    "We compute the Jaccard distance. In this step, we must first tokenize the sentences. We then take the **sets**; *unique values of those tokenized sentences*, lemmatize them and compute the **jaccard similarity as 1 - jaccard distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.4, 0.0, 0.033]\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "for key in d:\n",
    "        \n",
    "    w1 = set(d[key][0])\n",
    "    w2 = set(d[key][1])\n",
    "\n",
    "    # jaccard similarity 1 - jaccard distance\n",
    "    dist = jaccard_distance(w1, w2)\n",
    "    jaccard_similarity = 1 - dist\n",
    "    tests.append(round(jaccard_similarity,3))\n",
    "print(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare the results with gold standard by giving the pearson correlation between them.\n",
    "And now, we open the Golden Standard file and calculate the perason correlation with and without lemmatization. \n",
    "\n",
    "**Pearson Correlation**\n",
    "It shows the linear relationship between two sets of data. That means: the strength of the association between the two variables. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation\n",
    "\n",
    "**Coefficient Value** -- Strength of Association\n",
    "0.1 < | r | < .3 -- small correlation\n",
    "0.3 < | r | < .5 -- medium/moderate correlation\n",
    "| r | > .5 -- large/strong correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: -0.187\n"
     ]
    }
   ],
   "source": [
    "for line in open('./trial/STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "\n",
    "a = pearsonr(standard[0:6], tests[0:6])[0]\n",
    "print('Pearson correlation:', round(a,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
