{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandatory Exercise - Session 6\n",
    "### Olga Valls & Lavanya Mandadapu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.text import Text\n",
    "from nltk.wsd import lesk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to remove punctuation from the texts and convert them into lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    # Remove all the digits and punctuation from data\n",
    "    # decimal digit or not word+space = space\n",
    "    line = re.sub(r'(\\d|[^\\w ])', ' ', line)\n",
    "    # Remove spaces at the start of the sentence\n",
    "    # spaces at the start (^) (+: one or more repetitions)\n",
    "    line = re.sub(r'^[ ]+', '', line)\n",
    "    # Convert all the texts to lower case\n",
    "    line = line.lower()\n",
    "    # Replace continuous white spaces by a single one\n",
    "    line = re.sub(r'[ ]+', ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty lists to fill with the results of:\n",
    "- Jaccard distances\n",
    "- Similarities\n",
    "- Golden records (extracted from file \"STS.gs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccards = []\n",
    "similarities = []\n",
    "golden = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks 1-3:\n",
    "1. Read all pairs of sentences of the **trial set** within the evaluation framework of the project.\n",
    "2. Apply **Lesk’s algorithm** to the words in the sentences.\n",
    "3. Compute their **similarities** by considering senses and **Jaccard coefficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each line of the file\n",
    "- Split the sentences\n",
    "- Remove punctuation for each sentence and convert into lowercase\n",
    "- Tokenize to create a list of words\n",
    "- Create POS Tag pairs\n",
    "- Apply Lesk algorithm for Word Sense Disambiguation\n",
    "- Compute Jaccard Distance and Similarities between the sets of disambiguated synsets\n",
    "\n",
    "Jaccard is a similarity between to sets (intersection / union)\n",
    "As the python function is jaccard_distance(set1,set2), we compute the similarity:\n",
    "D = 1 / (1+S), where S = Jaccard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Line1: ['id1', 'The bird is bathing in the sink.', 'Birdie is washing itself in the water basin.']\n",
      "- Sentence1: ['the', 'bird', 'is', 'bathing', 'in', 'the', 'sink']\n",
      "- Sentence2: ['birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin']\n",
      "- Pairs 1: [('the', 'DT'), ('bird', 'NN'), ('is', 'VBZ'), ('bathing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN')]\n",
      "- Pairs 2: [('birdie', 'NN'), ('is', 'VBZ'), ('washing', 'VBG'), ('itself', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('water', 'NN'), ('basin', 'NN')]\n",
      "- Sentlesk 1: [Synset('bird.n.02'), Synset('be.v.12'), Synset('bathe.v.01'), Synset('sinkhole.n.01')]\n",
      "- Sentlesk 2: [Synset('shuttlecock.n.01'), Synset('be.v.12'), Synset('wash.v.09'), Synset('body_of_water.n.01'), Synset('washbasin.n.01')]\n",
      "> Jaccard Distance: 0.875\n",
      "> Similarities (1 / (1 + jd)): 0.5333333333333333\n",
      "** Line2: ['id2', 'In May 2010, the troops attempted to invade Kabul.', 'The US army invaded Kabul on May 7th last year, 2010.']\n",
      "- Sentence1: ['in', 'may', 'the', 'troops', 'attempted', 'to', 'invade', 'kabul']\n",
      "- Sentence2: ['the', 'us', 'army', 'invaded', 'kabul', 'on', 'may', 'th', 'last', 'year']\n",
      "- Pairs 1: [('in', 'IN'), ('may', 'MD'), ('the', 'DT'), ('troops', 'NNS'), ('attempted', 'VBD'), ('to', 'TO'), ('invade', 'VB'), ('kabul', 'NN')]\n",
      "- Pairs 2: [('the', 'DT'), ('us', 'PRP'), ('army', 'VBP'), ('invaded', 'JJ'), ('kabul', 'NN'), ('on', 'IN'), ('may', 'MD'), ('th', 'VB'), ('last', 'JJ'), ('year', 'NN')]\n",
      "- Sentlesk 1: [Synset('troop.n.02'), Synset('undertake.v.01'), Synset('invade.v.04'), Synset('kabul.n.01')]\n",
      "- Sentlesk 2: [Synset('kabul.n.01'), Synset('year.n.03')]\n",
      "> Jaccard Distance: 0.8\n",
      "> Similarities (1 / (1 + jd)): 0.5555555555555556\n",
      "** Line3: ['id3', 'John said he is considered a witness but not a suspect.', '\"He is not a suspect anymore.\"']\n",
      "- Sentence1: ['john', 'said', 'he', 'is', 'considered', 'a', 'witness', 'but', 'not', 'a', 'suspect']\n",
      "- Sentence2: ['he', 'is', 'not', 'a', 'suspect', 'anymore']\n",
      "- Pairs 1: [('john', 'NN'), ('said', 'VBD'), ('he', 'PRP'), ('is', 'VBZ'), ('considered', 'VBN'), ('a', 'DT'), ('witness', 'NN'), ('but', 'CC'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN')]\n",
      "- Pairs 2: [('he', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN'), ('anymore', 'RB')]\n",
      "- Sentlesk 1: [Synset('whoremaster.n.01'), Synset('suppose.v.01'), Synset('embody.v.02'), Synset('view.v.02'), Synset('witness.n.05'), Synset('not.r.01'), Synset('defendant.n.01')]\n",
      "- Sentlesk 2: [Synset('embody.v.02'), Synset('not.r.01'), Synset('defendant.n.01'), Synset('anymore.r.01')]\n",
      "> Jaccard Distance: 0.625\n",
      "> Similarities (1 / (1 + jd)): 0.6153846153846154\n",
      "** Line4: ['id4', 'They flew out of the nest in groups.', 'They flew into the nest together.']\n",
      "- Sentence1: ['they', 'flew', 'out', 'of', 'the', 'nest', 'in', 'groups']\n",
      "- Sentence2: ['they', 'flew', 'into', 'the', 'nest', 'together']\n",
      "- Pairs 1: [('they', 'PRP'), ('flew', 'VBD'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('in', 'IN'), ('groups', 'NNS')]\n",
      "- Pairs 2: [('they', 'PRP'), ('flew', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('together', 'RB')]\n",
      "- Sentlesk 1: [Synset('fly.v.12'), Synset('group.n.02')]\n",
      "- Sentlesk 2: [Synset('fly.v.10'), Synset('together.r.04')]\n",
      "> Jaccard Distance: 1.0\n",
      "> Similarities (1 / (1 + jd)): 0.5\n",
      "** Line5: ['id5', 'The woman is playing the violin.', 'The young lady enjoys listening to the guitar.']\n",
      "- Sentence1: ['the', 'woman', 'is', 'playing', 'the', 'violin']\n",
      "- Sentence2: ['the', 'young', 'lady', 'enjoys', 'listening', 'to', 'the', 'guitar']\n",
      "- Pairs 1: [('the', 'DT'), ('woman', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('the', 'DT'), ('violin', 'NN')]\n",
      "- Pairs 2: [('the', 'DT'), ('young', 'JJ'), ('lady', 'NN'), ('enjoys', 'VBZ'), ('listening', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('guitar', 'NN')]\n",
      "- Sentlesk 1: [Synset('woman.n.02'), Synset('be.v.05'), Synset('play.v.35'), Synset('violin.n.01')]\n",
      "- Sentlesk 2: [Synset('lady.n.03'), Synset('love.v.02'), Synset('heed.v.01'), Synset('guitar.n.01')]\n",
      "> Jaccard Distance: 1.0\n",
      "> Similarities (1 / (1 + jd)): 0.5\n",
      "** Line6: ['id6', 'John went horse back riding at dawn with a whole group of friends.', 'Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.']\n",
      "- Sentence1: ['john', 'went', 'horse', 'back', 'riding', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friends']\n",
      "- Sentence2: ['sunrise', 'at', 'dawn', 'is', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it']\n",
      "- Pairs 1: [('john', 'NN'), ('went', 'VBD'), ('horse', 'NN'), ('back', 'RB'), ('riding', 'VBG'), ('at', 'IN'), ('dawn', 'NN'), ('with', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('group', 'NN'), ('of', 'IN'), ('friends', 'NNS')]\n",
      "- Pairs 2: [('sunrise', 'NN'), ('at', 'IN'), ('dawn', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('magnificent', 'JJ'), ('view', 'NN'), ('to', 'TO'), ('take', 'VB'), ('in', 'IN'), ('if', 'IN'), ('you', 'PRP'), ('wake', 'VBP'), ('up', 'RP'), ('early', 'RB'), ('enough', 'RB'), ('for', 'IN'), ('it', 'PRP')]\n",
      "- Sentlesk 1: [Synset('toilet.n.01'), Synset('plump.v.04'), Synset('knight.n.02'), Synset('back.r.02'), Synset('ride.v.13'), Synset('dawn.n.01'), Synset('group.n.02'), Synset('friend.n.05')]\n",
      "- Sentlesk 2: [Synset('sunrise.n.03'), Synset('dawn.n.03'), Synset('be.v.12'), Synset('view.n.07'), Synset('take.v.34'), Synset('awaken.v.01'), Synset('up.r.05'), Synset('early_on.r.01'), Synset('enough.r.01')]\n",
      "> Jaccard Distance: 1.0\n",
      "> Similarities (1 / (1 + jd)): 0.5\n"
     ]
    }
   ],
   "source": [
    "txt_file = open('../00_data/trial/STS.input.txt', 'r')\n",
    "\n",
    "for i, line in enumerate(txt_file):\n",
    "    sentences = nltk.sent_tokenize(line)\n",
    "    temp = sentences[0].split('\\t')\n",
    "    temp.append(sentences[1])\n",
    "    # For each sentence, remove punctuation and convert to lowercase\n",
    "    sent = []\n",
    "    for t in temp:\n",
    "        pt = preprocess(t)\n",
    "        sent.append(pt)\n",
    "    print(\"** Line{}: {}\".format((i + 1), temp))\n",
    "\n",
    "    # tokenizer\n",
    "    words = [nltk.word_tokenize(s) for s in sent]\n",
    "    sentence1 = words[1]\n",
    "    sentence2 = words[2]\n",
    "    print('- Sentence1: {}'.format(sentence1))\n",
    "    print('- Sentence2: {}'.format(sentence2))\n",
    "\n",
    "    # Apply Lesk’s algorithm to the words in the sentences.\n",
    "    # POS tagging for each sentence\n",
    "    pairs1 = nltk.pos_tag(sentence1)\n",
    "    pairs2 = nltk.pos_tag(sentence2)\n",
    "    print('- Pairs 1: {}'.format(pairs1))\n",
    "    print('- Pairs 2: {}'.format(pairs2))\n",
    "\n",
    "    # Apply Lesk Algortithm to both sentences\n",
    "    temp1 = [lesk(sentence1, p[0], p[1][0].lower()) for p in pairs1]\n",
    "    temp2 = [lesk(sentence2, p[0], p[1][0].lower()) for p in pairs2]\n",
    "    # We keep those different from None (no duplicates)\n",
    "    sentlesk1 = [t for t in temp1 if t is not None]\n",
    "    sentlesk2 = [t for t in temp2 if t is not None]\n",
    "    print('- Sentlesk 1: {}'.format(sentlesk1))\n",
    "    print('- Sentlesk 2: {}'.format(sentlesk2))\n",
    "\n",
    "    # Compute their similarities by considering senses and Jaccard coefficient.\n",
    "    # jaccard_distance() of the Lesk WSD of each sentence\n",
    "    jd = jaccard_distance(set(sentlesk1), set(sentlesk2))\n",
    "    jaccards.append(jd)\n",
    "    print('> Jaccard Distance: {}'.format(jd))\n",
    "    # similarities: 1 / (1+S) (where S = Jaccard distance)\n",
    "    sim = 1 / (1 + jd)\n",
    "    sim2 = 1 - jd\n",
    "    similarities.append(sim)\n",
    "    print('> Similarities (1 / (1 + jd)): {}'.format(sim))\n",
    "    \n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard distances: [0.875, 0.8, 0.625, 1.0, 1.0, 1.0]\n",
      "Similarities (1 / (1 + jd)): [0.5333333333333333, 0.5555555555555556, 0.6153846153846154, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print('Jaccard distances: {}'.format(jaccards))\n",
    "print('Similarities (1 / (1 + jd)): {}'.format(similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being the <u>pairs of sentences</u>:<br>\n",
    "id1\t- The bird is bathing in the sink. - Birdie is washing itself in the water basin.<br>\n",
    "id2\t- In May 2010, the troops attempted to invade Kabul. - The US army invaded Kabul on May 7th last year, 2010.<br>\n",
    "id3\t- John said he is considered a witness but not a suspect. - \"He is not a suspect anymore.\" John said.<br>\n",
    "id4\t- They flew out of the nest in groups. - They flew into the nest together.<br>\n",
    "id5\t- The woman is playing the violin. - The young lady enjoys listening to the guitar.<br>\n",
    "id6\t- John went horse back riding at dawn with a whole group of friends. - Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.\n",
    "\n",
    "We have to take into account the <u>pre-processing</u> that we have made to the sentences:\n",
    "- remove punctuation\n",
    "- convert to lowercase\n",
    "For all cases, we have computed the Similarity as 1 / (1 + jd) , being jd: Jaccard Distance.\n",
    "\n",
    "From this <u>formula</u> we understand that:\n",
    "- when jd = 0, the similarity gets the maximum value, which is 1.\n",
    "- when jd = 1, the similarity gets the minimum value, which is 0.5\n",
    "Thus, two words/lemmas/sense that are equal should get a similarity value of 1; and 0.5 when they are totally different.\n",
    "\n",
    "If we can compare the results obtained, we can see that the values for words/lemmas are always higher than the values for senses.\n",
    "\n",
    "For **words** and **lemmas** we get the same results:<br>\n",
    "<u>Jaccard distances</u>: [0.7272727272727273, 0.8, 0.5454545454545454, 0.6, 0.9090909090909091, 0.8928571428571429<br>\n",
    "<u>Similarities (1 / (1 + jd))</u>: [0.5789473684210527, 0.5555555555555556, 0.6470588235294118, 0.625, 0.5238095238095238, 0.5283018867924528]<br>\n",
    "\n",
    "The <u>ranking</u> for the sentences would be: id3, id4, id1, id2, id6, id5\n",
    "\n",
    "For disambiguated **senses** we get the following results:<br>\n",
    "<u>Jaccard distances</u>: [0.875, 0.8, 0.625, 1.0, 1.0, 1.0]<br>\n",
    "<u>Similarities (1 / (1 + jd))</u>: [0.5333333333333333, 0.5555555555555556, 0.6153846153846154, 0.5, 0.5, 0.5]<br>\n",
    "\n",
    "The <u>ranking</u> for the sentences would be: id3, id2, id1, id4/id5/id6<br>\n",
    "\n",
    "Here we can see that, acording to Lesk, the last three pairs of sentences are totally different between them.\n",
    "\n",
    "id3 always get the best ranking position, id1 is always in the middle of the ranking, and id5 in the lower position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compare the results with gold standard by giving the pearson correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Golden Records file to extract the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden Records file\n",
    "golden_file = open('../00_data/trial/STS.gs.txt', 'r')\n",
    "\n",
    "for line in golden_file:\n",
    "    cols = nltk.sent_tokenize(line)\n",
    "    columnes = cols[0].split('\\t')\n",
    "    golden.append(columnes[1])\n",
    "\n",
    "golden_file.close()\n",
    "\n",
    "golden = list(map(int, golden))  # convert int strings into int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden Standards: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print('Golden Standards: {}'.format(golden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson Correlation**<br>\n",
    "It shows the linear relationship between two sets of data. That means: the strength of the association between the two variables.\n",
    "It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation\n",
    "\n",
    "Coefficient Value -- Strength of Association<br>\n",
    "0.1 < | r | < .3 -- small correlation<br>\n",
    "0.3 < | r | < .5 -- medium/moderate correlation<br>\n",
    "       | r | > .5 -- large/strong correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Pearson Correlation (with Similarities (1 / (1 + jd))): -0.5219919882301676\n"
     ]
    }
   ],
   "source": [
    "# Pearson correlation, with golden standards and similarities\n",
    "pearson_s = stats.pearsonr(similarities, golden)[0]\n",
    "print('==> Pearson Correlation (with Similarities (1 / (1 + jd))): {}'.format(pearson_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being the pairs of sentences:<br>\n",
    "id1\t- The bird is bathing in the sink. - Birdie is washing itself in the water basin.<br>\n",
    "id2\t- In May 2010, the troops attempted to invade Kabul. - The US army invaded Kabul on May 7th last year, 2010.<br>\n",
    "id3\t- John said he is considered a witness but not a suspect. - \"He is not a suspect anymore.\" John said.<br>\n",
    "id4\t- They flew out of the nest in groups. - They flew into the nest together.<br>\n",
    "id5\t- The woman is playing the violin. - The young lady enjoys listening to the guitar.<br>\n",
    "id6\t- John went horse back riding at dawn with a whole group of friends. - Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.\n",
    "\n",
    "According to **Golden Records file**, the ranking from \"most similar\" to \"less similar\" sentences would be:<br>\n",
    "id1, id2, id3, id4, id5, id6\n",
    "\n",
    "We have seen that we have been getting the following **Similarities**:\n",
    "- for <u>Words</u> and <u>Lemmas</u>: id3, id4, id1, id2, id6, id5\n",
    "- for <u>Senses</u>: id3, id2, id1, id4/id5/id6\n",
    "\n",
    "and the following **Pearson Correlations**:\n",
    "- for <u>Words</u> and <u>Lemmas</u>: 0.3902992700114095\n",
    "- for <u>Senses</u>: 0.5219919882301676\n",
    "\n",
    "For **words** and **lemmas** we have a <u>small/medium</u> correlation, whereas using Lesk for disambiguating of **senses** we have a <u>moderate</u> correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
