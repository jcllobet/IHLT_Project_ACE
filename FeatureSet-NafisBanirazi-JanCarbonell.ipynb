{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.641123Z",
     "start_time": "2018-12-07T23:28:26.802147Z"
    }
   },
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "\n",
    "#variable initialization and instantiation\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std_train = []\n",
    "gold_std_test = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.667188Z",
     "start_time": "2018-12-07T23:28:27.644328Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.684121Z",
     "start_time": "2018-12-07T23:28:27.669278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def preprocessing(line):\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    line = regex.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line)\n",
    "    line = regex.sub(r\"what's\", \"what is \", line)\n",
    "    line = regex.sub(r\"\\'s\", \" \", line)\n",
    "    line = regex.sub(r\"\\'ve\", \" have \", line)\n",
    "    line = regex.sub(r\"can't\", \"cannot \", line)\n",
    "    line = regex.sub(r\"n't\", \" not \", line)\n",
    "    line = regex.sub(r\"i'm\", \"i am \", line)\n",
    "    line = regex.sub(r\"\\'re\", \" are \", line)\n",
    "    line = regex.sub(r\"\\'d\", \" would \", line)\n",
    "    line = regex.sub(r\"\\'ll\", \" will \", line)\n",
    "    line = regex.sub(r\",\", \" \", line)\n",
    "    line = regex.sub(r\"\\.\", \" \", line)\n",
    "    line = regex.sub(r\"!\", \" ! \", line)\n",
    "    line = regex.sub(r\"\\/\", \" \", line)\n",
    "    line = regex.sub(r\"\\^\", \" ^ \", line)\n",
    "    line = regex.sub(r\"\\+\", \" + \", line)\n",
    "    line = regex.sub(r\"\\-\", \" - \", line)\n",
    "    line = regex.sub(r\"\\=\", \" = \", line)\n",
    "    line = regex.sub(r\"'\", \" \", line)\n",
    "    line = regex.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", line)\n",
    "    line = regex.sub(r\":\", \" : \", line)\n",
    "    line = regex.sub(r\" e g \", \" eg \", line)\n",
    "    line = regex.sub(r\" b g \", \" bg \", line)\n",
    "    line = regex.sub(r\" u s \", \" american \", line)\n",
    "    line = regex.sub(r\"\\0s\", \"0\", line)\n",
    "    line = regex.sub(r\" 9 11 \", \"911\", line)\n",
    "    line = regex.sub(r\"e - mail\", \"email\", line)\n",
    "    line = regex.sub(r\"j k\", \"jk\", line)\n",
    "    line = regex.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    #only accept alphanum\n",
    "    # [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    #remove punctuation\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.701984Z",
     "start_time": "2018-12-07T23:28:27.687526Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def stopwords_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "# convert words to tokens\n",
    "def pos_tag_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(pos_tag_to_lemmas, tokens))\n",
    "\n",
    "def pos_tag_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synset, Nammed Entity and Content Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.714197Z",
     "start_time": "2018-12-07T23:28:27.705705Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesking_sentence(pos_tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tagged_sentence]\n",
    "    result = []\n",
    "    \n",
    "    none_type_objects = []\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        # 'NoneType' object has no attribute 'name'\n",
    "        try:\n",
    "            result.append(lesk(sentence,word, wordnet_pos_code(tag)).name())\n",
    "        except:\n",
    "            result.append(word)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Potential Networks\n",
    "- MLPRegressor --> Using regressors\n",
    "- Support Vector Regressor\n",
    "- KNN Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.874826Z",
     "start_time": "2018-12-07T23:28:27.716837Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.894986Z",
     "start_time": "2018-12-07T23:28:27.877565Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemma(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    \n",
    "    #jaccard_similarity & edit similarity\n",
    "    #print ('jaccard:', float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), 'edit_dist:', float(edit_distance(lemmas_0, lemmas_1)))\n",
    "    return [float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))),float(edit_distance(lemmas_0, lemmas_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.904958Z",
     "start_time": "2018-12-07T23:28:27.897915Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lesk_0, lesk_1 = lesking_sentence(pos_0), lesking_sentence(pos_1)\n",
    "    \n",
    "    #jaccard_similarity\n",
    "    #print (float(1 - jaccard_distance(set(lesk_0), set(lesk_1))), '\\n')\n",
    "    return float(1 - jaccard_distance(set(lesk_0), set(lesk_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.914562Z",
     "start_time": "2018-12-07T23:28:27.907389Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_feature_generation(sent_0, send_1):\n",
    "    featureset = {}\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    \n",
    "    #jaccard_similarity & edit similarity\n",
    "    print ('jaccard:', float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), 'edit_dist:', float(edit_distance(lemmas_0, lemmas_1)))\n",
    "    return [float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))),float(edit_distance(lemmas_0, lemmas_1))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:28:27.918578Z",
     "start_time": "2018-12-07T23:28:27.916415Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('Training Lemmas')\n",
    "#training_data_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in train_input]\n",
    "#training_data_X_lemma_edit = [lemma(data[0], data[1])[1] for data in train_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:30:40.408428Z",
     "start_time": "2018-12-07T23:28:27.920955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data with Lemmas\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Training data with Lesk\n",
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Training!\n",
      "\n",
      "Testing data with Lemmas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Testing Lesk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Testing!\n",
      "\n",
      "Results\n"
     ]
    }
   ],
   "source": [
    "print('Training data with Lemmas')\n",
    "trn_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in train_input]\n",
    "trn_X_lemma_edit = [lemma(data[0], data[1])[1] for data in train_input]\n",
    "\n",
    "print('Training data with Lesk')\n",
    "trn_X_lesk = [lesk_jaccard(data[0], data[1]) for data in train_input]\n",
    "training_scores_y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "\n",
    "print('Testing data with Lemmas')\n",
    "tst_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in test_input]\n",
    "tst_X_lemma_edit = [lemma(data[0], data[1])[1] for data in test_input]\n",
    "\n",
    "print('Testing Lesk')\n",
    "tst_X_lesk = [lesk_jaccard(data[0], data[1])for data in test_input]\n",
    "\n",
    "testing_scores_y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "print('Results')\n",
    "#maybe label encoding https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous\n",
    "#print(training_data_X)\n",
    "#print('########################################################################\\n')\n",
    "#print(np.array(training_data_X).reshape(-1,1))\n",
    "#print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n')\n",
    "#print(training_scores_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:32:56.818231Z",
     "start_time": "2018-12-07T23:32:56.805056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>trainning lemma edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  trainning lemma edit\n",
       "0  0.473684                   7.0\n",
       "1  0.500000                   5.0\n",
       "2  0.357143                   7.0\n",
       "3  0.611111                   6.0\n",
       "4  0.111111                  14.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Pandas DataFrame\n",
    "df_X = pd.DataFrame(trn_X_lemma_jaccard)\n",
    "df_X[\"trainning lemma edit\"] = pd.DataFrame(trn_X_lemma_edit)\n",
    "#df_X = pd.DataFrame(training_data_X_lesk)\n",
    "#df_X[\"trainning lesk\"] = pd.DataFrame(training_data_X_lesk)\n",
    "\n",
    "df_y = pd.Series(training_scores_y)\n",
    "\n",
    "df_X_Test = pd.DataFrame(tst_X_lemma_jaccard)\n",
    "#df_X_Test = pd.DataFrame(testing_data_X_lesk)\n",
    "#df_X_Test[\"testing lesk\"] = pd.DataFrame(testing_data_X_lesk)\n",
    "df_X_Test[\"testing lemma edit\"] = pd.DataFrame(tst_X_lemma_edit)\n",
    "\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:43:39.750292Z",
     "start_time": "2018-12-07T23:43:37.725081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "MLP Training Accuracy:  0.758\n",
      "MLP Testing Accuracy:  0.64\n",
      "MLP Drop Train-Test:  0.118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "r = MLPRegressor(max_iter=1000)\n",
    "r.fit(df_X, df_y)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(training_scores_y, train_prediction)[0]\n",
    "b = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('MLP Training Accuracy: ',round(a,3))\n",
    "print('MLP Testing Accuracy: ', round(b,3))\n",
    "print('MLP Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "MLP Training Accuracy:  0.758\n",
      "MLP Testing Accuracy:  0.64\n",
      "MLP Drop Train-Test:  0.118\n"
     ]
    }
   ],
   "source": [
    "#using pytorch\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#activation tanh or logistic give better performance\n",
    "r = MLPRegressor(max_iter=1000, solver='lbfgs', hidden_layer_sizes=(100,50), activation='logistic')\n",
    "r.fit(df_X, df_y)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "d = pearsonr(training_scores_y, train_prediction)[0]\n",
    "e = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('MLP Training Accuracy: ',round(a,3))\n",
    "print('MLP Testing Accuracy: ', round(b,3))\n",
    "print('MLP Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:35:09.723517Z",
     "start_time": "2018-12-07T23:35:09.046929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "SVR Training Accuracy:  0.727\n",
      "SVR Testing Accuracy:  0.579\n",
      "SVR Drop Train-Test:  0.148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "r = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "r.fit(df_X, df_y)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "f = pearsonr(training_scores_y, train_prediction)[0]\n",
    "g = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('SVR Training Accuracy: ',round(f,3))\n",
    "print('SVR Testing Accuracy: ', round(g,3))\n",
    "print('SVR Drop Train-Test: ', round(f-g,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/IHLT_01/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Grid Searching SVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel':('linear', 'poly', 'sigmoid', 'rbf'), 'C':(0.1, 1, 10), 'epsilon':(0, 0.1, 0.2, 0.3,),  'tol':(0.0005, 0.001, 0.002, 0.003)}\n",
    "r = GridSearchCV(SVR(gamma = 'scale', degree = 4), parameters)\n",
    "\n",
    "\n",
    "r.fit(df_X, df_y)\n",
    "print (r.best_score_)\n",
    "print (r.best_params_)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "f = pearsonr(training_scores_y, train_prediction)[0]\n",
    "g = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('SVR Training Accuracy: ',round(f,3))\n",
    "print('SVR Testing Accuracy: ', round(g,3))\n",
    "print('SVR Drop Train-Test: ', round(f-g,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T23:35:09.769415Z",
     "start_time": "2018-12-07T23:35:09.726301Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "r = KNeighborsRegressor(n_neighbors=15)\n",
    "r.fit(df_X, df_y)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "h = pearsonr(training_scores_y, train_prediction)[0]\n",
    "i = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('KNN Training Accuracy: ',round(h,3))\n",
    "print('KNN Testing Accuracy: ', round(i,3))\n",
    "print('KNN Drop Train-Test: ', round(h-i,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
