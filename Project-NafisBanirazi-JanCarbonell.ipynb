{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 3\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#variable initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "# convert words to tokens\n",
    "def tokens_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(token_to_lemmas, tokens))\n",
    "\n",
    "def token_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Standarization\n",
    "### Regular Expression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-f2cb1af5dfad>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f2cb1af5dfad>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    jaccard distance\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jaccard distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Potential Networks\n",
    "- Knn\n",
    "- Perceptron\n",
    "- LinealRegression -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_jaccard(sent_0, sent_1):\n",
    "    #print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(tokens_0), tokens_to_lemmas(tokens_1)\n",
    "    \n",
    "    #jaccard_similarity\n",
    "    print (5 *float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), '\\n')\n",
    "    return 5*float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training')\n",
    "training_data_X = [lemma_jaccard(data[0], data[1]) for data in train_input]\n",
    "training_scores_Y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "\n",
    "print('Testing')\n",
    "testing_data_X = [lemma_jaccard(data[0], data[1])for data in test_input]\n",
    "testing_scores_Y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "print('Results')\n",
    "#maybe label encoding https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous\n",
    "print(training_data_X)\n",
    "print('########################################################################\\n')\n",
    "print(np.array(training_data_X).reshape(-1,1))\n",
    "print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n')\n",
    "print(training_scores_Y)\n",
    "\n",
    "#creating Knn object\n",
    "clf = KNeighborsClassifier(1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(training_data_X, training_scores_Y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "prediction = clf.predict(testing_data_X).tolist()\n",
    "\n",
    "for line in open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    gold_std.append(float(line[0]))\n",
    "\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(gold_std, prediction)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_pair_array in train_input:\n",
    "    sents = [nltk.word_tokenize(s) for s in sentence_pair_array]\n",
    "        \n",
    "    w1 = set(sents[0])\n",
    "    w2 = set(sents[1])\n",
    "    \n",
    "    wordtype1 = pos_tag(w1)\n",
    "    lem1 = set([lemmatize(w) for w in wordtype1])\n",
    "    \n",
    "    wordtype2 = pos_tag(w2)\n",
    "    lem2 = set([lemmatize(w) for w in wordtype2])\n",
    "    \n",
    "    jaccard_similarity = 1 - jaccard_distance(w1, w2)\n",
    "    jaccard_similarity_lem = 1 - jaccard_distance(lem1, lem2)\n",
    "    tests.append(round(jaccard_similarity,3))\n",
    "    tests_lem.append(round(jaccard_similarity_lem,3))\n",
    "print(tests_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we open the Golden Standard file and calculate the perason correlation with and without lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(float(line[0]))\n",
    "\n",
    "a = pearsonr(standard, tests)[0]\n",
    "b = pearsonr(standard, tests_lem)[0]\n",
    "print('Pearson correlation without lemmatization:', round(a,3))\n",
    "print('Pearson correlation with lemmatization:', round(b,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
