{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Student: Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/jcllobet/general/382f158355754599b4c5922d9e60737c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml in the top of the file for experiment tracking\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"WgXEAqBycAS6nrjJC5zkNTLA2\",\n",
    "                        project_name=\"general\", workspace=\"jcllobet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.778257Z",
     "start_time": "2018-12-11T01:01:53.997302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk import pos_tag\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "#variable initialization and instantiation\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std_train = []\n",
    "gold_std_test = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.800801Z",
     "start_time": "2018-12-11T01:01:54.781241Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.816260Z",
     "start_time": "2018-12-11T01:01:54.802577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def preprocessing(line):\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    line = regex.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line)\n",
    "    line = regex.sub(r\"what's\", \"what is \", line)\n",
    "    line = regex.sub(r\"\\'s\", \" \", line)\n",
    "    line = regex.sub(r\"\\'ve\", \" have \", line)\n",
    "    line = regex.sub(r\"can't\", \"cannot \", line)\n",
    "    line = regex.sub(r\"n't\", \" not \", line)\n",
    "    line = regex.sub(r\"i'm\", \"i am \", line)\n",
    "    line = regex.sub(r\"\\'re\", \" are \", line)\n",
    "    line = regex.sub(r\"\\'d\", \" would \", line)\n",
    "    line = regex.sub(r\"\\'ll\", \" will \", line)\n",
    "    line = regex.sub(r\",\", \" \", line)\n",
    "    line = regex.sub(r\"\\.\", \" \", line)\n",
    "    line = regex.sub(r\"!\", \" ! \", line)\n",
    "    line = regex.sub(r\"\\/\", \" \", line)\n",
    "    line = regex.sub(r\"\\^\", \" ^ \", line)\n",
    "    line = regex.sub(r\"\\+\", \" + \", line)\n",
    "    line = regex.sub(r\"\\-\", \" - \", line)\n",
    "    line = regex.sub(r\"\\=\", \" = \", line)\n",
    "    line = regex.sub(r\"'\", \" \", line) #careful, it used to be \" \". Testing again for accuracy purposes. \n",
    "    line = regex.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", line)\n",
    "    line = regex.sub(r\":\", \" : \", line)\n",
    "    line = regex.sub(r\" e g \", \" eg \", line)\n",
    "    line = regex.sub(r\" b g \", \" bg \", line)\n",
    "    line = regex.sub(r\" u s \", \" american \", line)\n",
    "    line = regex.sub(r\"\\0s\", \"0\", line)\n",
    "    line = regex.sub(r\" 9 11 \", \"911\", line)\n",
    "    line = regex.sub(r\"e - mail\", \"email\", line)\n",
    "    line = regex.sub(r\"j k\", \"jk\", line)\n",
    "    line = regex.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    #only accept alphanum\n",
    "    # [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    #remove punctuation\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "#initializing stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#testing that it works\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.830135Z",
     "start_time": "2018-12-11T01:01:54.819191Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def stopwords_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "# convert words to tokens\n",
    "def pos_tag_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Function to get wordnet pos code\n",
    "def treebank_pos_code(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('A'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(pos_tag_to_lemmas, tokens))\n",
    "\n",
    "def pos_tag_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]\n",
    "\n",
    "def tokens_to_stemming(tokens):\n",
    "    return list(stemmer.stem(token) for token in tokens)\n",
    "    #print(tokens_to_stemms(['a','running','verbose','singing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synset, Nammed Entity and Content Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.020Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesking_sentence(pos_tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tagged_sentence]\n",
    "    result = []\n",
    "    \n",
    "    none_type_objects = []\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        # 'NoneType' object has no attribute 'name'\n",
    "        try:\n",
    "            result.append(lesk(sentence,word, wordnet_pos_code(tag)).name())\n",
    "        except:\n",
    "            result.append(word)            \n",
    "    return result\n",
    "\n",
    "def get_stanford_named_entity_chunked(sentence):\n",
    "    \"\"\"Given the passed a tokenized sentence, returns an array with the chunks (words and named entities) it contains, using Stanford NLP\"\"\"\n",
    "    \n",
    "    tokenized_s = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # tag and run as a normal word or a named entity (e.g. a person or an organization)\n",
    "    tagged_s = tagger.tag(tokenized_s)\n",
    "    #print(tagged_s)\n",
    "    \n",
    "    chunked_sentence = []\n",
    "    last_token = ''\n",
    "    last_tag = ''\n",
    "    \n",
    "    for tagged_token in tagged_s:\n",
    "        \n",
    "        token = tagged_token[0]\n",
    "        tag = tagged_token[1]\n",
    "        \n",
    "        # make normal words have lower case, also discard punctuation marks\n",
    "        if tag == 'O':\n",
    "            if token.isalnum():\n",
    "                chunked_sentence.append(token.lower())\n",
    "         \n",
    "        # keep named entities with the original capitalization\n",
    "        else:\n",
    "            if last_tag == tag:\n",
    "                chunked_sentence[-1] += ' ' + token\n",
    "            else:\n",
    "                chunked_sentence.append(token)\n",
    "        \n",
    "        last_token = token\n",
    "        last_tag = tag\n",
    "    \n",
    "    return chunked_sentence\n",
    "\n",
    "# example: note it does not group the terms of named entities, always 1 by 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distances and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline\n",
    "\n",
    "#jaccard and edit distances\n",
    "def jaccard_d(list_0, list_1):\n",
    "    if len(set(list_0)) == 0 or len(set(list_1)) == 0:\n",
    "        return len(set(list_0).union(set(list_1)))\n",
    "    else:\n",
    "        return float(jaccard_distance(set(list_0),set(list_1)))\n",
    "    \n",
    "def edit_d(list_0, list_1):\n",
    "    return float(edit_distance(list_0,list_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.050Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_feature_generation(sent_0, sent_1):\n",
    "    #print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    stop_0, stop_1 = stopwords_from_sent(sent_0), stopwords_from_sent(sent_1)\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    #ner_0, ner_1 = get_stanford_named_entity_chunked(sent_0), get_stanford_named_entity_chunked(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    stems_0, stems_1 = tokens_to_stemming(token_0), tokens_to_stemming(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    lesk_0, lesk_1 = lesking_sentence(pos_0), lesking_sentence(pos_1)\n",
    "    \n",
    "\n",
    "    featureset = {\n",
    "        \"stops_jaccard\":jaccard_d(stop_0, stop_1),\n",
    "        \"stops_edit\":edit_d(stop_0, stop_1),\n",
    "        \"tokens_jaccard\":jaccard_d(token_0, token_1),\n",
    "        \"tokens_edit\":edit_d(token_0, token_1),\n",
    "        #\"ner_jaccard\":jaccard_d(ner_0, ner_1),\n",
    "        #\"ner_edit\":edit_d(ner_0, ner_1),\n",
    "        \"pos_jaccard\":jaccard_d(pos_0, pos_1),\n",
    "        \"pos_edit\":edit_d(pos_0, pos_1),\n",
    "        \"stems_jaccard\":jaccard_d(stems_0, stems_1),\n",
    "        \"stems_edit\":edit_d(stems_0, stems_1),\n",
    "        \"lemmas_jaccard\":jaccard_d(lemmas_0, lemmas_1),\n",
    "        \"lemmas_edit\":edit_d(lemmas_0, lemmas_1),\n",
    "        \"lesk_jaccard\":jaccard_d(lesk_0, lesk_1),\n",
    "        \"lesk_edit\":edit_d(lesk_0, lesk_1)\n",
    "    }\n",
    "    \n",
    "    #\"NER\":[] --> DO ZALANDO's IMPLEMENTATION https://github.com/zalandoresearch/flair\n",
    "   \n",
    "    \n",
    "    return featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Finished Training!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas_edit</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>lesk_edit</th>\n",
       "      <th>lesk_jaccard</th>\n",
       "      <th>pos_edit</th>\n",
       "      <th>pos_jaccard</th>\n",
       "      <th>stems_edit</th>\n",
       "      <th>stems_jaccard</th>\n",
       "      <th>stops_edit</th>\n",
       "      <th>stops_jaccard</th>\n",
       "      <th>tokens_edit</th>\n",
       "      <th>tokens_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lemmas_edit  lemmas_jaccard  lesk_edit  lesk_jaccard  pos_edit  \\\n",
       "0          7.0        0.526316        7.0      0.526316       7.0   \n",
       "1          5.0        0.500000        5.0      0.500000       5.0   \n",
       "2          7.0        0.642857        7.0      0.733333       7.0   \n",
       "3          6.0        0.388889        6.0      0.388889       6.0   \n",
       "4         14.0        0.888889       15.0      0.928571      16.0   \n",
       "\n",
       "   pos_jaccard  stems_edit  stems_jaccard  stops_edit  stops_jaccard  \\\n",
       "0     0.526316         7.0       0.526316         5.0       0.363636   \n",
       "1     0.500000         5.0       0.500000         3.0       0.800000   \n",
       "2     0.642857         7.0       0.642857         4.0       0.666667   \n",
       "3     0.388889         6.0       0.388889         3.0       0.333333   \n",
       "4     0.965517        14.0       0.888889         4.0       0.666667   \n",
       "\n",
       "   tokens_edit  tokens_jaccard  \n",
       "0          7.0        0.526316  \n",
       "1          5.0        0.500000  \n",
       "2          7.0        0.642857  \n",
       "3          6.0        0.388889  \n",
       "4         14.0        0.888889  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training data')\n",
    "X_train = [main_feature_generation(data[0], data[1]) for data in train_input]\n",
    "df_X_train = pd.DataFrame(X_train)\n",
    "training_scores_y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data\n",
      "Finished Testing!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas_edit</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>lesk_edit</th>\n",
       "      <th>lesk_jaccard</th>\n",
       "      <th>pos_edit</th>\n",
       "      <th>pos_jaccard</th>\n",
       "      <th>stems_edit</th>\n",
       "      <th>stems_jaccard</th>\n",
       "      <th>stops_edit</th>\n",
       "      <th>stops_jaccard</th>\n",
       "      <th>tokens_edit</th>\n",
       "      <th>tokens_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lemmas_edit  lemmas_jaccard  lesk_edit  lesk_jaccard  pos_edit  \\\n",
       "0          6.0        0.714286        8.0      0.941176       8.0   \n",
       "1          9.0        0.761905        9.0      0.761905      10.0   \n",
       "2         10.0        0.588235       10.0      0.500000      11.0   \n",
       "3          6.0        0.727273        6.0      0.727273       6.0   \n",
       "4         13.0        0.789474       13.0      0.789474      13.0   \n",
       "\n",
       "   pos_jaccard  stems_edit  stems_jaccard  stops_edit  stops_jaccard  \\\n",
       "0     0.875000         5.0       0.615385         5.0       0.625000   \n",
       "1     0.818182         9.0       0.761905         1.0       0.400000   \n",
       "2     0.666667        10.0       0.588235         2.0       0.400000   \n",
       "3     0.727273         5.0       0.600000         4.0       0.250000   \n",
       "4     0.789474        13.0       0.789474         4.0       0.666667   \n",
       "\n",
       "   tokens_edit  tokens_jaccard  \n",
       "0          7.0        0.800000  \n",
       "1         10.0        0.818182  \n",
       "2         10.0        0.588235  \n",
       "3          5.0        0.600000  \n",
       "4         13.0        0.789474  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing data')\n",
    "X_test = [main_feature_generation(data[0], data[1]) for data in test_input]\n",
    "df_X_test = pd.DataFrame(X_test)\n",
    "testing_scores_y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "df_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1854014 , -0.29878695,  0.06185393, ..., -0.11665755,\n",
       "         0.16169462, -0.40050658],\n",
       "       [-0.26212613, -0.4160811 , -0.34878997, ...,  0.22791104,\n",
       "        -0.27990169, -0.5245446 ],\n",
       "       [ 0.1854014 ,  0.22065857,  0.06185393, ...,  0.12262619,\n",
       "         0.16169462,  0.14880467],\n",
       "       ...,\n",
       "       [-1.38094494, -2.64466996, -1.37539972, ..., -0.40379804,\n",
       "        -1.38389245, -2.88126705],\n",
       "       [ 0.1854014 , -0.54717456,  0.26717588, ..., -0.0879435 ,\n",
       "         0.16169462, -0.66317533],\n",
       "       [-0.48588989,  1.0696448 , -0.55411192, ..., -0.40379804,\n",
       "        -0.50069984,  1.0466037 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(df_X_train)\n",
    "X_test_scaled = scaler.transform(df_X_test)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 3.75, 2.8, 3.4, 2.4, 1.333, 4.6, 3.8, 4.2, 2.6, 4.4, 4.2, 5.0, 5.0, 5.0, 3.4, 5.0, 4.0, 3.2, 5.0, 4.4, 3.6, 3.6, 0.8, 1.4, 3.2, 3.8, 4.2, 3.0, 3.6, 3.0, 3.0, 3.0, 4.8, 2.5, 2.25, 3.5, 3.5, 3.0, 3.0, 3.667, 1.667, 3.6, 3.8, 3.4, 3.4, 3.8, 2.8, 3.75, 4.4, 3.75, 2.75, 3.75, 2.25, 3.8, 3.2, 3.8, 4.8, 4.0, 1.4, 4.2, 1.8, 4.2, 2.0, 2.6, 3.25, 5.0, 3.0, 3.75, 4.25, 4.75, 3.25, 3.75, 3.25, 3.8, 5.0, 3.6, 3.8, 2.0, 3.4, 4.2, 3.4, 3.0, 3.0, 3.25, 2.75, 2.6, 3.4, 3.6, 3.6, 4.8, 4.0, 3.6, 3.4, 2.4, 4.222, 3.0, 3.0, 2.75, 4.0, 1.75, 1.75, 4.5, 3.25, 5.0, 3.0, 2.8, 2.667, 4.0, 4.4, 4.25, 4.176, 3.0, 2.5, 3.8, 4.6, 1.4, 2.8, 2.8, 4.2, 1.0, 4.2, 2.75, 2.5, 3.5, 4.75, 3.0, 3.5, 4.0, 2.0, 3.2, 3.6, 3.8, 5.0, 4.0, 3.0, 2.5, 3.0, 4.0, 2.0, 3.0, 2.625, 3.2, 4.0, 3.2, 2.4, 4.4, 3.6, 2.4, 3.6, 4.6, 4.4, 4.4, 4.6, 4.4, 2.6, 3.6, 4.2, 3.2, 2.0, 2.8, 1.6, 3.4, 4.0, 2.6, 2.8, 3.0, 3.6, 5.0, 3.4, 4.0, 4.0, 3.5, 4.5, 3.2, 3.8, 3.2, 4.0, 3.25, 3.0, 4.25, 1.5, 4.25, 4.25, 0.75, 3.0, 3.5, 3.75, 4.0, 1.75, 1.75, 3.75, 4.0, 2.0, 3.4, 1.4, 4.8, 2.0, 3.4, 2.4, 3.4, 4.8, 2.6, 2.4, 2.6, 2.8, 4.4, 3.2, 2.4, 4.0, 3.25, 4.0, 2.75, 4.0, 3.6, 3.6, 4.4, 3.8, 4.2, 3.6, 4.6, 3.6, 3.25, 4.0, 4.5, 2.5, 3.75, 4.0, 1.5, 3.75, 3.455, 5.0, 3.75, 4.0, 2.5, 0.25, 1.75, 5.0, 3.833, 3.75, 3.25, 2.75, 3.0, 4.4, 3.2, 4.6, 3.6, 4.4, 3.4, 4.056, 3.5, 5.0, 3.0, 3.75, 4.0, 4.0, 4.75, 3.75, 3.8, 4.0, 3.5, 3.0, 3.176, 4.2, 3.4, 4.6, 4.0, 4.0, 3.25, 3.643, 3.6, 4.0, 1.0, 3.4, 1.75, 3.0, 3.25, 2.75, 3.4, 3.6, 3.2, 3.2, 2.8, 2.4, 3.4, 3.6, 3.8, 3.4, 4.8, 3.8, 3.333, 2.5, 3.5, 1.9, 3.0, 3.6, 2.4, 3.2, 2.0, 3.8, 3.0, 3.692, 3.0, 3.0, 2.75, 3.25, 3.25, 3.75, 3.6, 3.5, 5.0, 3.75, 3.0, 4.0, 3.333, 3.333, 4.667, 3.333, 2.6, 2.4, 3.857, 1.8, 3.6, 2.4, 3.0, 2.0, 1.6, 4.0, 3.6, 3.8, 4.4, 3.0, 3.8, 3.0, 4.25, 1.273, 1.0, 3.0, 4.0, 3.5, 4.0, 3.25, 2.0, 1.0, 3.0, 2.6, 2.6, 4.6, 3.2, 3.0, 4.75, 2.75, 4.5, 2.75, 2.2, 3.0, 2.4, 3.0, 3.0, 4.0, 2.588, 3.6, 4.667, 3.333, 2.333, 3.333, 3.0, 2.25, 2.5, 3.25, 4.0, 4.5, 3.5, 4.75, 2.0, 3.6, 2.769, 3.4, 4.0, 3.5, 2.0, 4.0, 2.8, 1.4, 4.0, 3.2, 1.8, 3.4, 4.733, 1.6, 3.2, 4.8, 1.4, 2.6, 3.5, 3.75, 3.25, 1.75, 3.0, 3.5, 3.25, 0.5, 2.25, 4.0, 3.25, 4.25, 2.4, 2.4, 4.0, 3.0, 3.4, 3.8, 4.0, 3.2, 3.5, 2.5, 4.5, 4.5, 3.6, 2.8, 4.8, 0.8, 4.091, 2.25, 5.0, 3.75, 3.667, 2.667, 3.333, 3.0, 3.444, 3.8, 3.0, 3.6, 3.75, 5.0, 3.0, 3.25, 3.4, 3.8, 3.6, 3.4, 3.6, 2.4, 2.4, 2.4, 4.5, 0.889, 3.25, 4.75, 4.75, 4.0, 3.5, 5.0, 3.6, 3.0, 3.8, 3.0, 1.8, 4.8, 3.8, 3.6, 4.0, 4.6, 3.8, 4.8, 2.0, 3.2, 2.2, 4.0, 4.2, 2.0, 1.2, 4.0, 3.0, 0.8, 2.8, 3.0, 3.8, 4.4, 2.6, 1.4, 3.2, 1.6, 3.0, 3.273, 4.0, 3.25, 3.25, 5.0, 3.5, 3.417, 1.75, 2.7, 2.75, 3.75, 3.75, 4.111, 3.6, 4.0, 4.2, 3.75, 3.8, 4.4, 3.6, 4.6, 1.667, 4.0, 4.333, 3.0, 3.5, 3.5, 2.0, 3.75, 2.0, 4.333, 1.8, 2.667, 3.8, 3.2, 2.8, 2.2, 4.333, 2.0, 3.0, 2.333, 3.25, 2.25, 3.5, 2.25, 0.75, 4.0, 3.5, 5.0, 3.75, 3.75, 4.25, 4.5, 4.0, 3.25, 0.75, 2.75, 4.0, 3.4, 2.2, 3.4, 3.4, 2.6, 3.4, 3.6, 3.25, 5.0, 4.25, 3.4, 4.25, 3.25, 4.25, 3.75, 3.667, 4.333, 3.909, 3.933, 3.75, 3.75, 4.25, 4.25, 3.5, 4.75, 3.25, 2.75, 3.0, 3.8, 3.4, 3.857, 4.0, 3.4, 3.2, 3.0, 3.2, 3.6, 1.6, 3.8, 2.6, 1.6, 2.0, 3.6, 4.5, 4.0, 3.0, 2.5, 3.5, 3.0, 3.769, 3.4, 1.8, 3.0, 3.0, 3.4, 4.25, 3.25, 4.0, 4.0, 4.0, 3.625, 4.25, 3.5, 4.0, 3.2, 3.4, 4.0, 3.4, 2.8, 3.6, 4.8, 3.8, 2.4, 1.6, 3.2, 3.8, 4.2, 5.0, 3.4, 1.0, 3.333, 4.0, 3.333, 3.2, 4.0, 3.8, 2.2, 3.75, 3.0, 1.25, 3.25, 3.333, 2.0, 3.0, 2.667, 3.667, 3.667, 4.333, 1.333, 2.2, 1.2, 4.4, 3.4, 4.2, 3.0, 3.4, 4.8, 2.75, 4.0, 1.0, 4.25, 3.0, 3.5, 4.25, 4.0, 1.75, 3.067, 3.25, 3.75, 2.25, 2.5, 3.75, 3.25, 4.0, 4.0, 3.444, 2.75, 4.6, 2.8, 4.8, 0.6, 2.75, 2.75, 3.25, 3.25, 2.5, 2.25, 1.5, 3.0, 3.8, 4.308, 3.4, 3.2, 3.5, 3.75, 1.75, 1.25, 3.4, 1.2, 3.8, 3.2, 3.5, 4.75, 3.5, 2.0, 2.8, 1.4, 2.0, 4.8, 3.8, 4.6, 1.4, 3.6, 3.0, 1.6, 4.0, 2.4, 3.5, 0.75, 5.0, 5.0, 4.0, 3.0, 1.333, 3.667, 3.091, 3.8, 4.4, 4.8, 3.0, 2.667, 4.0, 3.0, 4.0, 5.0, 3.4, 3.4, 3.857, 2.75, 4.0, 1.5, 3.75, 2.75, 4.0, 1.5, 1.8, 4.0, 3.0, 3.6, 3.4, 3.0, 2.6, 3.2, 3.5, 3.25, 3.75, 3.25, 5.0, 5.0, 0.3, 0.6, 4.2, 3.6, 5.0, 2.75, 5.0, 3.75, 5.0, 1.25, 5.0, 4.25, 3.4, 1.6, 5.0, 3.6, 5.0, 2.6, 5.0, 3.2, 4.8, 5.0, 5.0, 2.2, 5.0, 4.25, 5.0, 2.5, 2.8, 2.8, 2.8, 2.8, 5.0, 4.8, 2.4, 4.2, 5.0, 5.0, 5.0, 3.75, 3.8, 2.0, 2.0, 1.583, 1.778, 2.0, 2.0, 2.0, 3.2, 3.8, 4.4, 5.0, 5.0, 3.5, 4.0, 3.5, 4.4, 3.8, 2.6, 3.0, 5.0, 5.0, 3.667, 2.333, 2.5, 5.0, 3.824, 4.0, 4.4, 0.2, 5.0, 5.0, 4.0, 1.6, 5.0, 4.6, 5.0, 2.375, 2.6, 2.6, 2.6, 2.2, 1.7, 3.4, 2.0, 4.0, 3.25, 4.5, 2.6, 2.0, 1.8, 1.556, 4.5, 4.0, 4.5, 5.0, 4.2, 1.6, 0.5, 1.4, 1.6, 4.4, 1.8, 2.2, 2.6, 2.4, 1.6, 3.0, 2.0, 2.0, 3.8, 4.0, 3.0, 4.4, 4.8, 4.8, 2.5, 2.0, 0.75, 0.727, 0.0, 1.25, 1.25, 1.25, 0.5, 3.25, 1.333, 1.0, 1.4, 1.333, 5.0, 4.0, 3.0, 4.4, 2.0, 2.167, 1.4, 1.2, 1.2, 2.2, 1.25, 5.0, 4.0, 4.0, 3.2, 2.0, 2.2, 3.2, 1.5, 4.75, 0.5, 0.067, 1.6, 2.6, 4.875, 5.0, 2.0, 1.4, 1.4, 3.6, 0.8, 3.8, 2.0, 1.6, 3.0, 3.615, 3.75, 0.5, 4.0, 4.0, 3.333, 1.5, 4.4, 4.857, 2.875, 0.8, 5.0, 2.2, 0.2, 0.4, 0.2, 1.8, 1.6, 1.3, 1.2, 1.2, 0.2, 1.2, 3.0, 3.5, 3.5, 2.0, 4.4, 5.0, 0.0, 3.2, 3.25, 0.0, 1.0, 3.667, 3.0, 4.2, 5.0, 4.091, 1.8, 2.0, 2.8, 1.0, 3.8, 4.0, 1.4, 1.8, 5.0, 0.4, 4.4, 1.0, 3.0, 1.2, 3.2, 2.6, 4.6, 3.4, 2.0, 4.0, 4.2, 3.8, 3.4, 3.8, 4.4, 4.2, 4.6, 1.2, 1.2, 0.8, 3.2, 4.0, 2.6, 4.4, 4.2, 3.2, 3.4, 4.0, 4.8, 0.2, 1.8, 2.6, 2.8, 5.0, 0.25, 3.75, 1.75, 3.5, 3.4, 1.4, 1.4, 1.0, 1.8, 4.6, 0.4, 3.6, 3.25, 0.778, 1.25, 0.75, 1.5, 1.0, 3.75, 3.25, 4.0, 3.0, 1.667, 3.333, 2.25, 0.5, 2.75, 2.0, 0.833, 3.75, 4.0, 3.25, 2.75, 1.0, 3.5, 3.75, 4.4, 0.4, 0.4, 4.2, 2.5, 0.25, 0.25, 0.25, 3.8, 2.6, 2.6, 2.0, 1.8, 0.0, 1.2, 3.8, 3.6, 1.8, 1.2, 3.2, 2.2, 0.8, 1.6, 2.8, 0.583, 3.2, 0.8, 3.8, 0.0, 0.667, 5.0, 4.333, 4.214, 1.8, 0.4, 2.0, 2.8, 0.8, 0.8, 0.4, 0.25, 0.75, 0.75, 1.0, 0.0, 0.8, 3.4, 2.769, 2.8, 3.2, 4.2, 2.4, 4.4, 1.8, 0.8, 1.4, 2.25, 0.25, 3.25, 4.5, 4.2, 0.8, 0.8, 0.8, 0.25, 3.5, 2.75, 4.25, 0.5, 0.5, 0.0, 0.5, 0.6, 2.2, 3.2, 4.4, 3.0, 2.25, 2.583, 0.0, 0.0, 1.8, 0.0, 1.4, 0.0, 0.4, 0.0, 0.0, 3.0, 2.0, 2.2, 1.2, 0.0, 1.0, 1.533, 1.667, 1.0, 0.308, 0.0, 0.2, 4.2, 2.6, 3.0, 3.8, 0.75, 0.25, 0.75, 3.0, 0.4, 0.4, 0.4, 4.2, 3.5, 3.5, 3.0, 3.75, 3.4, 3.4, 3.6, 3.8, 1.0, 1.4, 3.4, 3.6, 2.8, 4.0, 3.2, 3.4, 2.333, 2.667, 3.0, 4.667, 3.2, 1.6, 3.0, 0.2, 0.8, 1.2, 0.8, 0.8, 0.75, 0.0, 0.25, 3.25, 4.0, 3.0, 3.333, 3.333, 2.0, 3.0, 3.75, 4.0, 3.0, 2.0, 3.75, 0.0, 0.4, 0.4, 3.2, 0.4, 2.0, 0.0, 0.0, 0.0, 0.5, 4.5, 0.5, 0.5, 0.5, 4.0, 3.75, 4.25, 3.75, 2.75, 3.25, 3.0, 0.8, 0.4, 0.4, 0.4, 0.25, 0.25, 0.75, 0.25, 4.8, 0.8, 0.0, 3.0, 0.75, 3.75, 2.5, 0.0, 0.25, 0.0, 3.5, 3.25, 0.4, 0.8, 1.8, 1.6, 1.2, 4.2, 0.2, 0.2, 0.5, 0.0, 4.5, 0.0, 2.5, 0.75, 4.0, 2.75, 3.4, 3.4, 0.4, 0.4, 3.2, 2.4, 3.0, 0.4, 0.5, 3.929, 3.0, 1.75, 0.0, 0.4, 2.2, 2.6, 3.5, 0.5, 3.5, 0.0, 0.0, 0.0, 1.0, 2.0, 1.2, 0.6, 0.8, 4.0, 0.5, 2.5, 2.75, 3.25, 2.0, 0.8, 0.0, 0.4, 0.6, 0.8, 0.8, 0.8, 0.4, 3.0, 0.4, 0.0, 0.0, 0.0, 0.4, 0.0, 3.6, 0.231, 3.2, 0.0, 0.5, 0.3, 1.25, 0.0, 2.75, 5.0, 0.75, 1.25, 0.0, 0.0, 0.75, 3.75, 2.333, 5.0, 0.0, 3.2, 0.75, 0.5, 2.5, 1.75, 4.0, 0.0, 1.333, 4.6, 4.0, 1.2, 4.2, 3.8, 3.0, 0.2, 3.0, 2.2, 1.4, 0.0, 2.8, 3.8, 3.8, 4.4, 0.0, 3.0, 0.5, 2.75, 1.75, 0.0, 0.0, 1.0, 1.8, 1.6, 2.333, 2.0, 0.333, 1.0, 0.0, 0.0, 3.0, 1.5, 0.4, 0.4, 2.8, 0.167, 0.5, 3.25, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.25, 0.25, 4.25, 0.118, 1.0, 0.4, 0.0, 0.0, 2.5, 0.75, 3.0, 1.6, 2.6, 2.6, 0.0, 0.0, 0.0, 4.25, 0.0, 0.0, 1.0, 0.0, 3.5, 2.4, 2.6, 0.6, 0.1, 0.0, 3.75, 0.0, 0.0, 0.4, 0.4, 2.6, 2.0, 1.75, 3.75, 0.0, 0.75, 0.0, 0.0, 4.0, 0.0, 0.8, 3.4, 3.1, 0.0, 2.75, 2.0, 0.25, 2.692, 3.333, 3.0, 0.667, 3.5, 4.429, 5.0, 3.2, 0.333, 3.333, 3.333, 1.0, 0.188, 2.2, 0.0, 0.0, 0.4, 0.0, 0.25, 2.25, 0.25, 0.8, 2.6, 1.2, 3.2, 1.4, 0.364, 0.2, 2.75, 3.2, 0.6, 3.8, 3.4, 1.5, 2.5, 0.0, 5.0, 4.333, 1.333, 0.333, 3.333, 4.2, 0.8, 1.0, 0.0, 2.8, 1.5, 0.0, 0.0, 0.6, 1.2, 0.0, 0.0, 3.0, 0.0, 0.6, 1.0, 1.75, 0.0, 0.0, 0.0, 1.0, 0.0, 3.5, 0.0, 0.0, 0.0, 2.75, 2.5, 3.4, 2.2, 1.8, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 4.2, 4.25, 4.8, 4.8, 4.0, 4.8, 3.75, 4.8, 4.0, 4.8, 4.8, 4.2, 4.5, 4.0, 4.0, 4.0, 3.8, 5.0, 4.8, 4.8, 4.4, 3.5, 4.8, 3.0, 5.0, 4.4, 3.0, 3.4, 3.6, 4.6, 4.4, 4.2, 4.8, 4.2, 3.6, 4.4, 4.6, 4.6, 4.0, 4.8, 4.4, 5.0, 5.0, 4.0, 4.8, 3.6, 5.0, 4.2, 5.0, 4.2, 4.857, 4.667, 4.0, 4.4, 4.4, 4.6, 3.8, 2.8, 4.6, 4.6, 3.25, 4.0, 4.25, 5.0, 4.8, 4.4, 4.2, 3.4, 4.25, 4.75, 5.0, 4.25, 4.6, 3.2, 4.8, 4.6, 3.2, 4.0, 4.0, 5.0, 4.8, 4.2, 4.0, 4.8, 3.8, 5.0, 4.2, 4.0, 4.9, 4.4, 4.2, 4.4, 1.0, 4.2, 3.2, 5.0, 4.6, 4.0, 5.0, 5.0, 4.6, 4.2, 3.8, 4.8, 4.6, 5.0, 4.4, 4.4, 4.4, 5.0, 3.571, 3.6, 3.6, 4.4, 4.8, 4.0, 4.4, 5.0, 4.6, 2.4, 1.4, 4.6, 4.5, 3.75, 5.0, 4.0, 4.4, 4.8, 4.2, 3.8, 3.8, 4.2, 4.2, 3.6, 4.25, 4.0, 4.75, 4.6, 4.4, 4.4, 4.2, 3.4, 4.0, 3.6, 4.4, 4.0, 3.8, 5.0, 4.6, 3.2, 4.0, 0.0, 3.8, 5.0, 4.2, 4.2, 4.6, 5.0, 4.4, 1.8, 2.8, 5.0, 4.857, 4.0, 5.0, 3.8, 3.4, 4.4, 4.6, 4.8, 3.8, 0.0, 5.0, 4.6, 5.0, 4.6, 3.5, 3.75, 3.75, 5.0, 3.6, 4.6, 4.0, 5.0, 4.8, 4.4, 4.6, 4.8, 4.4, 4.2, 4.4, 4.8, 4.8, 4.8, 4.8, 4.8, 5.0, 4.8, 5.0, 4.6, 3.4, 3.6, 4.8, 4.2, 5.0, 4.0, 5.0, 5.0, 4.2, 4.4, 4.2, 5.0, 5.0, 4.4, 4.0, 5.0, 4.8, 4.0, 5.0, 4.4, 4.375, 3.6, 4.75, 4.5, 4.5, 4.5, 4.6, 5.0, 4.6, 4.6, 3.75, 5.0, 3.75, 4.25, 4.6, 4.6, 0.0, 4.0, 4.6, 4.6, 5.0, 4.2, 4.8, 4.4, 4.4, 4.8, 4.8, 4.6, 5.0, 4.6, 4.6, 4.6, 4.8, 4.8, 4.4, 5.0, 4.2, 5.0, 5.0, 4.8, 4.8, 4.0, 5.0, 3.4, 4.0, 4.4, 4.4, 4.6, 4.5, 3.2, 4.857, 4.0, 4.111, 4.6, 4.2, 5.0, 4.0, 4.2, 4.8, 5.0, 4.4, 3.8, 3.6, 5.0, 4.8, 4.6, 4.2, 4.4, 4.833, 4.8, 5.0, 2.8, 3.2, 3.8, 4.0, 0.0, 4.8, 4.2, 3.6, 4.8, 4.2, 3.2, 1.6, 4.2, 4.4, 5.0, 4.0, 4.4, 4.2, 5.0, 4.0, 4.2, 5.0, 4.0, 5.0, 4.6, 5.0, 4.8, 3.8, 3.4, 5.0, 2.6, 3.4, 3.2, 3.8, 5.0, 4.0, 4.4, 3.4, 4.2, 4.6, 4.0, 4.0, 4.0, 0.0, 2.8, 4.4, 4.2, 4.6, 4.167, 4.2, 3.2, 4.2, 4.6, 3.2, 4.6, 4.8, 4.4, 4.4, 4.667, 4.4, 4.8, 3.8, 4.8, 4.0, 4.6, 4.6, 5.0, 4.75, 4.8, 4.75, 4.833, 4.8, 4.6, 4.714, 4.6, 4.2, 4.8, 3.6, 4.6, 5.0, 4.6, 4.6, 4.6, 4.2, 4.6, 4.2, 4.2, 4.25, 4.5, 4.0, 5.0, 4.2, 4.6, 4.0, 5.0, 4.5, 4.5, 4.25, 4.75, 4.25, 3.75, 4.0, 4.5, 5.0, 3.6, 4.4, 5.0, 3.0, 4.0, 4.2, 4.2, 4.571, 4.6, 4.2, 4.6, 4.8, 4.6, 4.8, 4.8, 4.4, 5.0, 4.6, 4.0, 4.2, 5.0, 4.5, 4.6, 5.0, 4.25, 4.0, 3.25, 5.0, 5.0, 3.6, 5.0, 4.5, 5.0, 4.75, 3.2, 4.0, 4.6, 3.8, 4.2, 3.8, 4.0, 4.6, 4.4, 4.6, 4.4, 4.4, 3.25, 3.75, 4.25, 4.25, 4.2, 4.8, 3.4, 4.6, 4.6, 4.8, 3.8, 4.6, 4.8, 4.2, 4.0, 4.0, 4.75, 3.5, 4.4, 3.8, 4.0, 4.6, 5.0, 4.6, 4.4, 4.8, 4.6, 4.8, 4.0, 4.4, 4.8, 4.4, 4.8, 4.2, 4.6, 4.8, 4.4, 4.2, 4.8, 4.8, 4.8, 4.2, 4.6, 3.6, 3.6, 4.4, 4.6, 4.6, 4.8, 4.8, 4.4, 4.6, 4.4, 4.0, 4.8, 4.4, 4.4, 4.4, 2.0, 4.4, 4.8, 4.8, 4.4, 4.0, 4.0, 4.2, 4.25, 3.8, 4.4, 4.4, 4.6, 4.8, 3.8, 4.0, 3.4, 4.4, 5.0, 5.0, 4.2, 4.6, 4.2, 4.4, 4.4, 4.4, 4.6, 4.4, 4.4, 4.6, 2.8, 4.6, 4.8, 4.25, 4.8, 3.6, 5.0, 4.4, 4.4, 4.2, 4.6, 0.0, 2.4, 4.4, 4.2, 2.8, 4.8, 4.8, 3.4, 4.6, 4.0, 4.4, 4.4, 4.6, 4.0, 3.4, 4.2, 5.0, 4.4, 4.2, 4.2, 3.8, 3.6, 4.4, 4.6, 3.2, 4.4, 4.6, 4.8, 3.8, 4.8, 4.6, 4.8, 4.2, 4.4, 4.8, 4.6, 2.5, 4.8, 4.4, 5.0, 3.8, 4.4, 4.4, 4.4, 3.4, 5.0, 3.8, 4.6, 4.6, 4.8, 5.0, 4.2, 4.4, 4.2, 4.8, 5.0, 4.8, 5.0, 4.8, 4.0, 5.0, 5.0, 4.4, 4.6, 4.4, 4.6, 4.2, 4.0, 4.6, 3.6, 4.4, 3.6, 4.8, 4.4, 4.6, 5.0, 4.8, 4.6, 4.6, 4.4, 4.8, 4.8, 4.8, 4.2, 4.4, 4.6, 4.2, 4.4, 4.2, 5.0, 5.0, 3.2, 5.0, 3.4, 5.0, 5.0, 5.0, 4.2, 3.8, 4.6, 3.2, 3.4, 4.0, 5.0, 3.8, 4.4, 4.4, 4.6, 4.4, 4.8, 4.2, 4.6, 5.0, 4.4, 4.2, 4.4, 3.0, 4.8, 4.2, 4.4, 4.2, 4.8, 4.6, 4.4, 4.6, 3.8, 4.6, 4.8, 4.8, 3.8, 4.2, 4.4, 4.4, 4.6, 4.6, 4.6, 4.4, 4.6, 4.4, 4.6, 4.4, 5.0, 4.6, 4.6, 5.0, 4.8, 5.0, 5.0, 4.2, 4.4, 4.0, 4.8, 3.6, 3.75, 4.75, 3.5, 4.5, 3.0, 3.8, 0.0, 4.8, 4.8, 4.2, 4.6, 5.0, 4.8, 5.0, 3.8, 4.8, 5.0, 3.8, 4.6, 4.6, 5.0, 4.4, 4.8, 4.4, 4.2, 4.2, 5.0, 4.6, 4.2, 3.8, 5.0, 4.6, 3.6, 4.8, 5.0, 5.0, 5.0, 4.8, 5.0, 4.8, 0.8]\n",
      "[4.4, 0.8, 3.6, 3.4, 1.4, 4.6, 1.4, 3.6, 2.0, 5.0, 3.6, 4.4, 4.8, 4.6, 1.4, 2.6, 2.0, 2.8, 3.5, 3.2, 4.0, 4.25, 3.75, 5.0, 4.0, 4.0, 3.5, 2.5, 2.0, 4.4, 3.6, 4.2, 3.4, 4.0, 4.0, 4.8, 3.6, 3.4, 3.6, 3.4, 3.75, 4.25, 4.25, 3.25, 3.846, 1.2, 3.2, 4.0, 3.4, 3.2, 3.8, 3.2, 4.0, 3.333, 3.667, 2.0, 2.6, 3.0, 2.4, 2.6, 4.4, 2.8, 3.0, 4.2, 3.2, 3.2, 3.0, 2.6, 2.5, 4.5, 3.75, 3.25, 0.8, 3.2, 4.0, 2.2, 2.8, 3.6, 2.6, 3.8, 3.2, 5.0, 4.0, 1.2, 5.0, 3.8, 3.8, 3.4, 2.2, 4.2, 2.818, 3.2, 3.0, 2.4, 3.6, 3.875, 3.0, 4.4, 2.8, 1.6, 3.2, 5.0, 3.8, 4.4, 3.8, 3.8, 5.0, 3.2, 4.2, 3.0, 3.0, 3.8, 2.667, 2.333, 3.0, 1.333, 3.8, 3.4, 2.0, 3.4, 4.0, 2.8, 3.2, 3.6, 3.8, 3.6, 4.2, 4.0, 4.75, 3.75, 3.25, 3.533, 2.75, 2.5, 1.25, 1.75, 4.25, 4.0, 5.0, 4.25, 4.75, 2.75, 3.25, 3.5, 3.6, 3.2, 3.4, 4.6, 3.333, 3.333, 3.0, 4.0, 3.5, 3.5, 1.75, 3.0, 3.4, 3.6, 3.6, 2.0, 3.6, 1.6, 4.6, 4.4, 3.8, 2.6, 3.6, 3.0, 2.4, 1.2, 3.4, 3.0, 4.5, 3.5, 1.5, 3.0, 2.0, 3.4, 2.2, 1.8, 4.0, 3.5, 3.5, 2.75, 2.6, 2.2, 4.6, 3.0, 3.5, 4.75, 1.5, 4.75, 3.75, 3.5, 3.4, 1.5, 3.4, 3.2, 2.2, 2.4, 3.0, 3.0, 1.8, 2.4, 3.533, 4.0, 4.25, 4.75, 3.333, 5.0, 4.333, 3.0, 3.0, 3.0, 3.8, 1.846, 3.0, 3.0, 3.2, 1.0, 3.0, 2.647, 3.25, 3.25, 3.75, 3.25, 3.0, 4.75, 4.6, 3.929, 3.4, 1.75, 3.2, 3.8, 0.8, 4.6, 3.2, 2.2, 4.4, 4.4, 3.333, 4.0, 3.0, 1.333, 4.2, 3.2, 3.2, 3.0, 3.25, 4.25, 2.25, 3.5, 2.2, 3.4, 3.0, 3.0, 2.5, 1.75, 2.75, 4.25, 1.8, 2.8, 3.0, 5.0, 3.25, 4.0, 3.75, 1.25, 3.25, 2.0, 3.333, 3.0, 3.4, 5.0, 4.4, 3.4, 3.75, 5.0, 3.75, 4.0, 3.25, 2.25, 1.5, 3.0, 2.25, 3.75, 4.0, 4.75, 5.0, 3.75, 1.5, 3.067, 3.5, 2.75, 3.5, 3.25, 3.75, 3.8, 4.0, 3.2, 4.8, 2.2, 2.2, 4.6, 3.25, 3.25, 1.333, 2.5, 3.25, 3.75, 2.25, 2.75, 2.5, 4.75, 3.5, 3.25, 3.25, 3.0, 1.5, 2.0, 3.0, 3.0, 4.75, 4.778, 3.2, 3.6, 3.2, 3.2, 4.4, 4.364, 3.8, 4.4, 4.5, 4.0, 1.5, 3.0, 5.0, 3.6, 2.8, 1.8, 3.4, 2.6, 2.2, 4.8, 3.0, 2.0, 3.5, 3.75, 3.5, 3.333, 4.25, 4.0, 2.0, 1.6, 3.6, 3.4, 4.25, 4.75, 1.75, 3.25, 3.0, 2.4, 3.6, 3.4, 3.25, 3.5, 3.25, 3.786, 0.8, 2.4, 3.2, 4.923, 3.8, 2.6, 4.0, 3.0, 3.2, 3.2, 3.4, 3.2, 3.4, 2.6, 4.571, 3.4, 2.25, 3.5, 2.75, 4.75, 2.5, 3.25, 4.5, 3.5, 3.167, 4.5, 4.75, 2.75, 3.2, 2.6, 4.4, 3.0, 0.944, 2.4, 3.8, 3.8, 4.0, 3.8, 3.8, 4.2, 4.4, 4.2, 2.6, 3.056, 4.75, 3.5, 3.25, 1.5, 3.4, 3.75, 2.6, 2.8, 2.4, 3.2, 4.0, 3.6, 3.4, 2.4, 3.4, 3.6, 3.4, 3.4, 4.818, 3.2, 1.4, 3.0, 4.2, 3.2, 1.8, 3.8, 4.6, 2.8, 4.25, 3.0, 0.75, 4.0, 3.6, 2.2, 2.8, 3.2, 1.25, 3.0, 3.25, 3.0, 2.2, 3.0, 4.6, 3.6, 4.0, 3.0, 3.75, 3.714, 5.0, 3.2, 3.2, 1.6, 3.333, 4.0, 3.0, 1.333, 3.6, 4.2, 1.8, 3.231, 3.0, 3.0, 3.75, 5.0, 3.4, 3.2, 3.4, 2.8, 2.8, 3.6, 2.8, 3.0, 4.6, 3.2, 3.0, 3.8, 3.0, 4.5, 3.75, 3.0, 4.727, 3.8, 3.0, 4.6, 4.4, 1.0, 2.0, 2.8, 1.5, 3.25, 1.75, 1.75, 5.0, 3.6, 3.2, 3.0, 2.75, 4.5, 1.0, 3.75, 3.0, 4.5, 1.5, 3.0, 2.0, 4.0, 3.2, 3.0, 3.5, 4.0, 3.0, 3.25, 3.4, 2.0, 2.8, 4.0, 3.6, 3.4, 2.8, 3.2, 2.2, 3.2, 3.4, 3.0, 4.75, 3.25, 1.0, 4.25, 5.0, 1.6, 5.0, 3.8, 3.5, 4.75, 3.0, 3.5, 3.5, 3.0, 4.5, 3.25, 4.25, 3.25, 3.75, 4.25, 1.2, 2.6, 5.0, 3.0, 3.6, 3.0, 4.8, 1.8, 4.0, 3.8, 2.4, 2.8, 3.2, 3.0, 3.4, 3.8, 3.5, 3.25, 1.5, 3.692, 2.8, 3.2, 2.4, 2.0, 3.8, 3.4, 3.0, 4.6, 3.25, 3.75, 3.5, 4.5, 3.667, 3.333, 1.0, 1.0, 4.0, 5.0, 5.0, 3.4, 3.5, 3.5, 1.25, 3.5, 3.75, 2.25, 4.75, 3.25, 3.25, 3.25, 3.75, 3.25, 4.8, 3.8, 3.4, 2.0, 3.6, 4.4, 4.6, 3.6, 3.75, 3.5, 2.5, 3.25, 3.333, 3.333, 3.0, 2.667, 2.2, 3.2, 4.6, 3.2, 5.0, 3.6, 3.2, 3.2, 4.6, 3.8, 3.8, 3.4, 3.4, 3.4, 3.6, 3.6, 2.6, 2.2, 3.6, 3.8, 3.8, 4.0, 1.733, 4.0, 2.8, 3.2, 1.8, 3.0, 3.75, 3.75, 3.25, 2.909, 3.4, 3.6, 2.2, 2.6, 3.8, 1.6, 3.8, 3.4, 2.5, 3.75, 3.167, 2.5, 4.25, 2.75, 2.75, 3.25, 1.8, 4.2, 3.2, 3.2, 2.75, 1.75, 3.5, 3.75, 4.8, 3.4, 0.8, 4.0, 4.0, 3.75, 3.25, 1.25, 3.5, 4.75, 1.0, 2.5, 3.438, 4.0, 3.6, 4.2, 2.6, 3.2, 4.4, 3.0, 3.25, 2.0, 3.25, 4.5, 3.4, 3.0, 1.8, 1.0, 2.75, 3.25, 3.75, 2.75, 1.0, 3.6, 3.2, 1.6, 3.0, 3.2, 4.6, 2.8, 4.2, 3.2, 4.0, 3.0, 2.2, 2.8, 2.6, 5.0, 2.0, 3.25, 3.75, 3.5, 2.75, 2.75, 3.25, 2.25, 3.25, 3.0, 4.0, 2.0, 3.182, 3.0, 3.75, 3.0, 4.6, 2.6, 3.2, 4.4, 1.25, 2.5, 5.0, 5.0, 4.75, 5.0, 3.8, 3.8, 2.6, 2.4, 2.75, 4.25, 2.615, 4.25, 0.5, 1.6, 2.2, 5.0, 5.0, 4.2, 4.6, 3.867, 4.667, 1.667, 3.75, 2.333, 2.5, 5.0, 0.5, 3.75, 3.8, 5.0, 3.2, 2.8, 4.6, 3.6, 3.0, 5.0, 5.0, 3.2, 4.8, 1.583, 5.0, 5.0, 4.2, 5.0, 4.2, 5.0, 4.0, 4.0, 4.909, 4.909, 3.0, 0.8, 2.4, 4.2, 3.4, 5.0, 3.75, 2.75, 5.0, 4.0, 2.4, 3.6, 1.6, 4.2, 1.75, 5.0, 1.5, 5.0, 1.0, 1.0, 2.375, 3.8, 3.2, 3.2, 1.8, 4.4, 3.5, 3.75, 4.75, 4.0, 3.2, 1.556, 2.2, 3.938, 5.0, 5.0, 4.0, 1.6, 0.636, 4.75, 3.5, 3.0, 2.2, 1.4, 1.4, 1.714, 1.714, 1.714, 3.2, 4.0, 5.0, 5.0, 3.833, 0.6, 0.6, 2.917, 4.2, 4.4, 2.0, 2.6, 1.6, 2.0, 4.2, 2.0, 4.8, 4.4, 5.0, 3.0, 4.25, 4.25, 3.8, 2.4, 2.167, 1.6, 2.0, 2.0, 1.6, 1.8, 4.4, 4.0, 2.2, 4.4, 3.6, 3.6, 3.6, 0.5, 0.8, 3.6, 0.6, 1.2, 1.0, 2.6, 2.0, 2.2, 2.4, 3.6, 2.4, 1.917, 2.2, 4.8, 0.2, 1.643, 1.75, 4.25, 2.25, 4.0, 4.2, 4.8, 3.2, 3.0, 4.0, 4.4, 4.4, 4.6, 3.8, 4.8, 4.857, 5.0, 2.533, 2.25, 2.0, 0.75, 1.0, 1.0, 2.2, 1.0, 2.0, 0.6, 0.143, 2.0, 1.6, 0.8, 1.6, 2.2, 3.4, 4.0, 2.6, 4.8, 2.5, 5.0, 1.75, 1.0, 5.0, 1.4, 4.6, 4.0, 3.8, 3.2, 4.0, 4.0, 4.8, 0.6, 4.75, 2.2, 3.0, 0.0, 5.0, 2.2, 0.4, 4.8, 4.8, 4.8, 4.8, 3.8, 3.8, 3.0, 4.0, 5.0, 5.0, 5.0, 4.2, 3.8, 1.4, 3.0, 4.4, 3.6, 3.8, 3.0, 2.8, 1.4, 0.667, 4.0, 4.25, 3.75, 4.133, 4.0, 1.6, 3.6, 1.2, 1.6, 4.0, 4.0, 3.4, 3.2, 0.533, 1.0, 1.0, 0.6, 0.4, 0.4, 3.4, 1.2, 3.6, 5.0, 3.0, 3.0, 4.0, 1.2, 0.538, 0.6, 1.6, 1.4, 2.6, 3.6, 0.25, 0.25, 3.75, 3.5, 3.765, 2.75, 2.25, 2.75, 3.8, 4.8, 3.6, 1.2, 3.0, 0.4, 2.4, 3.6, 0.0, 0.5, 3.941, 0.5, 3.75, 4.0, 4.0, 3.0, 4.2, 4.5, 2.0, 5.0, 3.0, 3.8, 0.0, 0.0, 0.25, 4.0, 3.25, 0.75, 1.5, 0.5, 1.5, 3.5, 3.0, 3.8, 4.8, 0.2, 0.8, 0.8, 0.8, 4.2, 3.8, 0.6, 0.4, 5.0, 0.8, 1.2, 2.8, 3.2, 5.0, 4.4, 0.0, 1.0, 0.25, 0.0, 1.75, 3.75, 0.4, 0.4, 0.4, 0.6, 4.0, 1.4, 0.4, 0.4, 0.4, 0.4, 1.2, 4.8, 0.6, 0.8, 0.6, 2.0, 0.8, 0.8, 0.133, 3.2, 3.2, 0.6, 0.6, 0.0, 1.4, 4.0, 3.8, 2.2, 0.267, 3.6, 3.4, 3.111, 0.6, 0.4, 0.0, 1.2, 3.5, 4.25, 3.0, 4.5, 5.0, 5.0, 5.0, 3.75, 2.8, 0.2, 0.4, 3.8, 1.286, 4.6, 0.6, 2.0, 5.0, 3.8, 1.8, 0.0, 2.4, 2.4, 0.4, 0.0, 4.2, 3.8, 0.0, 0.0, 4.0, 0.8, 1.4, 1.5, 0.75, 2.5, 2.5, 3.5, 3.4, 3.4, 3.8, 3.8, 4.25, 2.812, 1.5, 0.5, 0.0, 0.0, 0.8, 0.0, 0.2, 0.85, 3.4, 0.4, 1.75, 3.25, 0.75, 4.25, 3.0, 4.0, 0.0, 0.0, 1.0, 0.5, 3.75, 4.25, 1.8, 3.2, 3.6, 0.0, 2.6, 3.2, 0.4, 3.8, 2.4, 2.8, 4.6, 0.0, 0.25, 3.923, 2.75, 0.0, 3.0, 3.8, 4.0, 4.6, 2.6, 3.6, 3.6, 2.8, 4.75, 3.75, 1.154, 1.0, 2.75, 1.25, 2.75, 0.0, 0.0, 0.0, 2.75, 3.75, 2.8, 2.6, 2.4, 2.2, 0.0, 0.2, 0.0, 2.8, 4.0, 0.0, 0.0, 0.75, 5.0, 0.0, 3.4, 3.2, 3.5, 1.75, 1.25, 0.833, 0.0, 3.75, 2.75, 0.0, 5.0, 3.8, 0.2, 3.8, 3.4, 1.4, 3.8, 0.8, 0.4, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 3.8, 2.25, 2.25, 2.75, 4.5, 2.4, 2.8, 3.2, 1.6, 0.4, 0.0, 2.6, 0.4, 0.6, 1.0, 3.8, 4.6, 4.25, 0.0, 1.5, 3.5, 2.8, 0.0, 0.0, 0.0, 0.333, 0.333, 3.333, 4.333, 1.6, 0.8, 0.8, 0.2, 0.0, 0.0, 0.0, 1.333, 3.0, 2.6, 2.8, 0.8, 1.0, 0.0, 0.0, 0.25, 1.0, 3.4, 0.6, 0.0, 0.5, 3.75, 5.0, 1.75, 2.667, 2.333, 0.4, 0.0, 0.0, 1.0, 0.8, 1.4, 0.0, 3.0, 4.0, 0.0, 0.2, 3.2, 5.0, 4.4, 0.75, 2.0, 2.0, 0.0, 2.8, 3.0, 3.091, 3.538, 2.75, 2.5, 2.5, 2.25, 3.6, 1.2, 0.2, 1.0, 0.4, 2.8, 0.8, 0.0, 3.5, 3.0, 3.0, 2.8, 0.0, 0.0, 2.2, 2.0, 3.75, 4.0, 4.75, 3.5, 0.8, 0.8, 4.0, 1.2, 1.75, 3.0, 0.5, 0.25, 0.0, 0.0, 0.0, 4.5, 3.8, 3.2, 3.2, 0.8, 1.5, 2.75, 1.5, 3.75, 3.2, 0.8, 0.0, 0.0, 2.4, 1.4, 3.2, 2.2, 2.533, 3.4, 0.0, 0.8, 0.0, 3.2, 0.0, 0.0, 0.0, 3.0, 2.8, 0.0, 0.0, 0.417, 0.0, 0.4, 2.5, 1.0, 2.0, 0.0, 0.5, 0.0, 2.5, 2.0, 2.6, 0.0, 3.6, 0.0, 0.6, 2.818, 3.0, 0.0, 1.6, 1.25, 3.533, 3.8, 3.25, 4.0, 4.0, 0.0, 1.0, 3.4, 3.6, 2.6, 1.4, 2.0, 3.2, 3.6, 0.0, 0.0, 0.0, 2.8, 0.0, 0.0, 0.0, 3.8, 0.643, 0.0, 2.25, 1.25, 3.4, 0.0, 0.2, 0.5, 0.0, 1.0, 4.4, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8, 2.4, 1.2, 0.6, 0.25, 3.25, 3.75, 1.25, 0.4, 1.8, 4.0, 3.25, 2.5, 0.75, 3.25, 2.5, 1.2, 0.083, 0.0, 3.8, 3.4, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 3.5, 3.75, 4.5, 4.0, 0.25, 0.2, 0.4, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 4.5, 5.0, 4.25, 4.5, 5.0, 5.0, 4.667, 5.0, 5.0, 5.0, 4.0, 3.5, 4.75, 3.75, 4.25, 5.0, 4.333, 4.5, 5.0, 5.0, 4.0, 5.0, 4.75, 5.0, 5.0, 5.0, 4.75, 4.5, 4.75, 5.0, 4.5, 4.25, 4.75, 4.25, 5.0, 5.0, 4.5, 5.0, 5.0, 5.0, 4.25, 5.0, 5.0, 4.0, 4.5, 4.75, 4.8, 4.25, 5.0, 5.0, 5.0, 5.0, 5.0, 4.75, 4.25, 4.75, 5.0, 4.25, 5.0, 5.0, 4.5, 4.5, 3.0, 5.0, 4.75, 4.0, 4.8, 4.8, 4.5, 5.0, 5.0, 5.0, 5.0, 4.75, 5.0, 5.0, 5.0, 5.0, 4.8, 4.6, 3.0, 4.75, 4.571, 5.0, 4.714, 5.0, 5.0, 5.0, 4.833, 5.0, 5.0, 4.0, 3.5, 5.0, 4.5, 4.75, 5.0, 4.25, 5.0, 5.0, 4.5, 3.75, 5.0, 4.667, 4.667, 4.0, 4.25, 5.0, 4.667, 4.5, 4.75, 4.429, 5.0, 4.2, 4.75, 5.0, 4.75, 4.429, 4.25, 3.5, 4.5, 5.0, 3.0, 4.5, 5.0, 5.0, 4.0, 5.0, 4.571, 5.0, 5.0, 4.8, 5.0, 5.0, 4.25, 4.429, 4.571, 4.75, 5.0, 5.0, 5.0, 3.0, 4.25, 5.0, 4.0, 5.0, 4.0, 4.0, 4.4, 4.25, 5.0, 3.4, 4.75, 3.4, 4.8, 4.75, 4.75, 3.0, 4.25, 5.0, 3.25, 4.75, 4.5, 4.667, 4.167, 4.5, 4.5, 4.5, 5.0, 4.5, 3.25, 4.667, 4.75, 4.75, 5.0, 4.8, 4.25, 4.5, 3.5, 4.5, 5.0, 4.667, 4.75, 4.0, 4.0, 4.5, 4.429, 4.25, 4.4, 4.6, 3.833, 5.0, 5.0, 5.0, 4.0, 3.714, 4.333, 5.0, 5.0, 5.0, 5.0, 5.0, 4.5, 5.0, 5.0, 4.167, 4.5, 4.167, 4.5, 4.25, 5.0, 4.2, 4.0, 4.5, 3.5, 4.167, 5.0, 3.75, 4.5, 4.75, 3.25, 3.25, 5.0, 5.0, 5.0, 5.0, 3.25, 5.0, 5.0, 4.25, 5.0, 5.0, 4.5, 4.75, 5.0, 5.0, 4.5, 4.5, 5.0, 4.5, 4.75, 4.25, 4.25, 5.0, 4.75, 5.0, 4.0, 4.75, 5.0, 4.25, 4.167, 4.75, 4.25, 5.0, 4.5, 4.5, 4.25, 3.833, 4.0, 4.75, 4.75, 4.167, 4.571, 4.0, 5.0, 4.75, 4.25, 4.25, 3.75, 4.5, 4.75, 5.0, 5.0, 4.5, 4.5, 4.25, 5.0, 4.75, 4.75, 4.714, 5.0, 4.0, 4.667, 4.25, 5.0, 5.0, 4.667, 4.8, 4.75, 4.571, 4.333, 4.667, 5.0, 3.714, 4.25, 1.5, 4.75, 5.0, 4.5, 5.0, 5.0, 5.0, 4.75, 4.167, 5.0, 4.6, 4.714, 3.75, 5.0, 5.0, 5.0, 5.0, 4.667, 4.8, 3.0, 4.5, 5.0, 5.0, 4.5, 4.5, 4.167, 5.0, 4.75, 5.0, 5.0, 3.75, 4.429, 5.0, 3.833, 5.0, 5.0, 4.0, 4.0, 4.429, 4.75, 4.25, 4.0, 4.8, 4.8, 4.0, 4.0, 4.667, 5.0, 4.75, 5.0, 4.75, 5.0, 4.4, 5.0, 5.0, 5.0, 5.0, 4.714, 3.714, 4.25, 4.75, 4.0, 4.0, 5.0, 4.6, 5.0, 5.0, 5.0, 3.75, 5.0, 4.75, 4.167, 4.5, 5.0, 5.0, 4.571, 4.25, 4.0, 5.0, 3.0, 2.75, 5.0, 5.0, 5.0, 4.0, 4.5, 4.5, 5.0, 2.75, 4.75, 5.0, 4.167, 4.25, 4.5, 4.75, 4.5, 4.75, 2.5, 5.0, 4.833, 5.0, 4.333, 4.75, 3.0, 4.5, 4.5, 5.0, 4.571, 5.0, 3.5, 4.167, 5.0, 5.0, 5.0, 4.0, 4.75, 3.5, 4.0, 4.75, 4.75, 4.75, 4.4, 5.0, 4.4, 4.667, 4.5, 4.5, 4.5, 4.667, 3.75, 3.75, 5.0, 5.0, 4.667, 4.25, 4.25, 5.0, 4.75, 5.0, 4.5, 3.0, 5.0, 5.0, 4.25, 4.0, 5.0, 5.0, 4.429, 4.25, 4.429, 4.25, 4.714, 5.0, 5.0, 4.8, 4.75, 4.5, 4.25, 4.25, 5.0, 4.75, 5.0, 4.0, 3.833, 5.0, 3.25, 3.25, 4.0, 3.25, 4.0, 3.333, 4.75, 0.5, 4.75, 3.75, 4.0, 1.5, 2.5, 4.25, 3.5, 4.25, 5.0, 3.75, 2.75, 3.5, 3.5, 3.5, 4.25, 2.75, 2.25, 3.75, 3.0, 2.75, 2.5, 2.25, 2.5, 3.5, 1.25, 0.0, 2.0, 0.75, 4.25, 0.0, 4.0, 3.25, 4.333, 5.0, 3.5, 4.75, 3.0, 3.0, 3.75, 2.75, 2.25, 2.5, 4.25, 4.0, 4.5, 3.75, 3.25, 4.25, 3.75, 1.0, 1.0, 3.75, 4.0, 2.75, 3.75, 1.5, 3.25, 4.0, 4.5, 2.0, 1.5, 4.0, 3.75, 2.333, 2.5, 4.5, 3.0, 2.75, 3.0, 3.0, 1.75, 4.0, 4.0, 3.75, 0.5, 2.5, 1.5, 2.8, 3.4, 0.0, 1.6, 4.0, 3.25, 3.5, 3.5, 2.5, 2.25, 3.333, 2.25, 3.25, 4.0, 3.75, 3.5, 5.0, 0.5, 3.5, 2.0, 2.5, 3.5, 3.25, 4.5, 2.75, 3.25, 2.0, 4.0, 0.75, 2.5, 4.25, 2.0, 2.25, 2.75, 3.5, 2.25, 4.0, 2.0, 0.75, 4.0, 3.5, 2.0, 2.5, 2.5, 4.0, 2.75, 2.25, 3.5, 1.0, 4.5, 2.5, 2.5, 2.75, 3.75, 4.0, 3.75, 1.75, 0.75, 2.75, 4.5, 3.75, 3.5, 3.75, 4.0, 2.2, 3.2, 2.8, 2.8, 4.5, 2.0, 1.0, 2.75, 3.75, 4.25, 2.25, 3.5, 0.5, 3.25, 0.0, 2.0, 2.25, 2.25, 4.0, 3.0, 3.5, 3.25, 4.0, 0.5, 1.75, 4.0, 3.25, 3.0, 4.25, 2.75, 3.0, 3.75, 2.0, 1.6, 1.0, 3.8, 2.25, 3.0, 1.75, 1.75, 3.4, 4.0, 4.4, 2.6, 2.25, 2.75, 3.25, 2.75, 4.5, 3.25, 4.25, 2.0, 3.25, 2.75, 3.75, 2.25, 3.25, 3.75, 3.75, 3.75, 1.5, 3.75, 3.0, 3.5, 3.5, 3.25, 4.0, 4.75, 3.75, 3.75, 2.5, 2.25, 3.667, 3.75, 3.25, 4.0, 4.0, 1.25, 4.5, 4.25, 3.25, 2.5, 4.25, 3.0, 2.25, 3.0, 2.25, 3.333, 2.75, 0.0, 4.75, 2.0, 3.0, 4.5, 0.75, 2.0, 3.75, 3.75, 4.75, 0.5, 5.0, 4.75, 3.25, 4.5, 4.75, 4.5, 4.75, 3.0, 2.5, 4.5, 3.75, 2.5, 3.5, 4.75, 4.5, 4.0, 4.0, 4.0, 4.75, 4.25, 3.5, 3.75, 4.5, 4.0, 4.0, 5.0, 3.5, 4.25, 4.0, 4.25, 4.25, 4.5, 4.0, 5.0, 5.0, 3.333, 2.75, 3.5, 2.0, 4.0, 3.5, 4.75, 3.5, 4.0, 3.5, 3.75, 4.75, 4.5, 4.0, 3.5, 3.0, 3.0, 4.75, 4.25, 4.25, 5.0, 5.0, 4.75, 4.75, 5.0, 3.25, 4.5, 5.0, 4.5, 4.25, 3.75, 4.0, 3.5, 4.25, 4.75, 4.0, 3.0, 3.5, 3.75, 4.5, 4.0, 3.25, 4.75, 4.25, 4.0, 5.0, 4.75, 4.25, 4.25, 4.75, 4.0, 4.0, 5.0, 3.0, 3.25, 4.75, 4.0, 2.25, 3.75, 4.25, 4.75, 3.75, 4.75, 4.75, 4.0, 3.5, 4.5, 3.25, 3.5, 3.75, 4.0, 3.75, 4.0, 3.75, 3.75, 4.5, 4.0, 3.75, 4.0, 3.5, 1.75, 3.75, 4.0, 3.5, 3.0, 5.0, 4.75, 5.0, 4.75, 2.75, 4.5, 3.75, 4.75, 4.25, 3.5, 4.5, 4.25, 4.0, 4.25, 4.25, 3.75, 4.5, 3.25, 3.25, 4.5, 4.5, 4.25, 4.0, 3.25, 4.0, 4.25, 4.25, 3.5, 3.5, 4.5, 3.75, 3.75, 4.5, 4.75, 4.25, 3.75, 4.5, 3.5, 4.0, 3.75, 4.5, 4.5, 4.0, 4.75, 2.5, 3.75, 3.5, 4.5, 4.0, 4.0, 4.25, 3.75, 2.25, 3.0, 4.5, 4.0, 4.75, 4.75, 4.667, 4.0, 4.333, 4.667, 4.667, 4.333, 4.25, 4.0, 4.25, 4.75, 3.75, 3.25, 3.5, 4.25, 4.0, 3.25, 5.0, 4.0, 3.5, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.75, 4.75, 4.0, 4.5, 4.0, 3.0, 3.75, 3.75, 5.0, 3.5, 4.5, 5.0, 4.75, 3.75, 5.0, 4.25, 4.75, 4.5, 3.75, 4.75, 4.0, 3.5, 4.0, 4.0, 3.75, 4.0, 4.5, 5.0, 4.25, 4.25, 4.5, 3.5, 3.75, 2.75, 2.75, 3.25, 3.25, 4.5, 3.75, 3.0, 3.5, 4.75, 3.5, 4.75, 4.75, 4.5, 4.25, 4.75, 4.75, 4.75, 5.0, 3.75, 5.0, 4.75, 4.25, 5.0, 5.0, 5.0, 4.0, 5.0, 5.0, 4.0, 5.0, 5.0, 4.0, 3.75, 3.75, 3.25, 5.0, 5.0, 5.0, 4.0, 5.0, 3.5, 4.0, 3.75, 4.25, 4.5, 4.75, 5.0, 5.0, 5.0, 5.0, 4.75, 4.25, 4.0, 5.0, 3.75, 4.5, 4.75, 5.0, 5.0, 4.25, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.75, 4.75, 4.75, 5.0, 5.0, 4.4, 4.8, 4.6, 4.6, 3.5, 3.5, 4.75, 4.25, 3.75, 5.0, 5.0, 5.0, 4.5, 4.25, 4.75, 4.25, 4.0, 4.0, 3.75, 4.0, 5.0, 5.0, 4.25, 5.0, 4.75, 4.25, 5.0, 5.0, 4.75, 5.0, 5.0, 5.0, 4.333, 3.75, 4.25, 4.75, 5.0, 4.0, 5.0, 4.5, 3.667, 3.667, 4.333, 5.0, 4.5, 4.0, 4.75, 4.5, 5.0, 5.0, 4.5, 5.0, 4.333, 5.0, 5.0, 5.0, 4.75, 4.5, 5.0, 4.75, 5.0, 5.0, 4.5, 5.0, 4.25, 4.75, 4.5, 5.0, 4.0, 5.0, 5.0, 4.75, 4.0, 4.5, 4.5, 4.5, 5.0, 4.0, 4.25, 5.0, 3.75, 4.75, 4.75, 4.0, 4.25, 4.25, 4.5, 4.25, 4.25, 4.5, 4.75, 3.75, 4.5, 4.5, 4.75, 4.5, 4.0, 5.0, 4.5, 4.5, 5.0, 3.75, 3.75, 5.0, 4.25, 4.75, 4.25, 4.5, 4.75, 4.0, 5.0, 4.75, 5.0, 4.25, 4.75, 5.0, 4.5, 4.5, 5.0, 3.25, 5.0, 5.0, 5.0, 5.0, 5.0, 4.75, 4.75, 4.75, 4.25, 4.0, 4.75, 4.5, 5.0, 5.0, 5.0, 4.25, 4.0, 4.5, 4.5, 5.0, 3.75, 5.0, 4.5, 4.75, 5.0, 4.25, 4.25, 5.0, 4.5, 3.5, 4.0, 5.0, 4.5, 5.0, 4.25, 5.0, 4.6, 5.0, 4.4, 5.0, 4.75, 5.0, 5.0, 5.0, 4.25, 4.75, 5.0, 5.0, 4.25, 4.25, 5.0, 4.75, 3.25, 5.0, 4.75, 4.75, 4.25, 4.75, 4.75, 4.25, 4.5, 4.0, 4.5, 3.25, 4.5, 4.25, 4.5, 4.25, 3.5, 4.5, 5.0, 3.75, 4.0, 5.0, 5.0, 4.667, 4.5, 5.0, 4.5, 5.0, 4.0, 4.5, 4.0, 3.75, 2.2, 4.5, 4.75, 4.75, 4.0, 5.0, 0.5, 4.5, 5.0, 4.25, 4.75, 5.0, 4.75, 3.25, 4.75, 4.25, 4.75, 4.75, 4.5, 5.0, 5.0, 4.5, 3.0, 3.4, 4.5, 3.8, 2.833, 4.75, 4.75, 4.5, 5.0, 4.25, 5.0, 3.6, 0.5, 2.2, 5.0, 5.0, 4.667, 4.75, 4.25, 5.0, 5.0, 4.75, 4.5, 4.75, 4.5, 2.5, 1.75, 2.75, 2.25, 5.0, 3.75, 4.0, 4.0, 4.75, 5.0, 5.0, 4.5, 4.25, 5.0, 5.0, 4.75, 4.5, 5.0, 5.0, 4.8, 5.0, 3.4, 5.0, 5.0, 5.0, 5.0, 3.75, 5.0, 4.6, 4.75, 4.8, 4.0, 3.75, 4.714, 5.0, 4.6, 3.2, 4.5, 4.0, 3.0, 4.6, 2.25, 4.0, 4.4, 2.833, 2.75, 4.75, 5.0, 5.0, 5.0, 3.75, 4.25, 4.75, 5.0, 4.5, 4.4, 5.0, 4.75, 4.6, 3.75, 4.75, 4.75, 5.0, 2.25, 4.5, 4.75, 4.75, 4.8, 4.25, 2.5, 5.0, 5.0, 5.0, 3.5, 4.25, 4.25, 4.0, 0.25, 4.25, 5.0, 3.75, 3.75, 5.0, 2.75, 4.75, 1.75, 4.5, 3.5, 4.5, 4.75, 5.0, 5.0, 4.75, 4.0, 5.0, 5.0, 4.5, 5.0, 4.75, 5.0, 5.0, 4.75, 4.25, 4.25, 4.75, 5.0, 4.5, 3.5, 2.75, 3.75, 3.5, 5.0, 4.5, 4.75, 5.0, 4.4, 4.25, 4.5, 4.75, 4.25, 3.8, 4.5, 4.5, 4.25, 5.0, 4.25, 4.5, 4.5, 4.75, 4.75, 4.75, 2.5, 4.5, 3.5, 4.4, 4.25, 3.75, 4.75, 4.75, 4.75, 4.25, 5.0, 2.833, 5.0, 5.0, 4.75, 4.75, 3.5, 5.0, 4.75, 4.0, 5.0, 4.25, 4.8, 5.0, 4.667, 5.0, 4.25, 4.0, 5.0, 0.25, 5.0, 3.6, 4.25, 4.75, 4.25, 5.0, 5.0, 5.0, 4.714, 3.75, 4.6, 4.5, 4.75, 2.0, 5.0, 3.75, 5.0, 5.0, 4.75, 5.0, 4.75, 4.0, 5.0, 4.714, 3.25, 4.75, 4.0, 3.0, 4.5, 4.5, 5.0, 5.0, 4.75, 3.75, 4.0, 4.5, 4.0, 4.75, 4.25, 4.75, 4.0, 2.75, 5.0, 2.25, 3.75, 5.0, 3.5, 4.75, 4.714, 3.5, 3.25, 5.0, 5.0, 5.0, 3.75, 4.0, 5.0, 4.25, 3.75, 4.0, 3.5, 4.8, 4.0, 5.0, 4.25, 5.0, 4.8, 4.75, 4.5, 4.0, 5.0, 4.6, 4.75, 5.0, 5.0, 4.5, 4.5, 5.0, 3.5, 4.75, 3.5, 4.5, 4.25, 5.0, 2.5, 4.25, 4.25, 4.4, 4.75, 4.8, 3.75, 5.0, 4.75, 4.75, 4.2, 5.0, 4.6, 4.75, 4.75, 4.5, 3.25, 4.4, 4.5, 4.5, 4.0, 4.333, 5.0, 5.0, 5.0, 4.75, 4.75, 5.0, 4.0, 4.75, 4.5, 4.5, 4.25, 4.75, 5.0, 4.75, 4.25, 3.6, 3.6, 2.75, 5.0, 3.75, 4.2, 3.5, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.75, 4.25, 5.0, 4.5, 3.25, 5.0, 4.5, 4.5, 4.8, 5.0, 4.5, 5.0, 4.75, 5.0, 4.75, 5.0, 4.0, 4.75, 5.0, 3.75, 4.75, 2.25, 5.0, 4.5, 4.5, 4.667, 5.0, 4.25, 4.667, 5.0, 4.75, 5.0, 4.0, 4.5, 2.25, 3.75, 5.0, 3.0, 4.75, 4.75, 4.5, 1.0, 3.25, 3.5]\n"
     ]
    }
   ],
   "source": [
    "what_changed = \"Implemented Keras\"\n",
    "print(training_scores_y)\n",
    "print(testing_scores_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:542: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-397328e7aa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline: %.2f%% (%.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m                     )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m                     )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "# X_train_scaled\n",
    "# X_test_scaled\n",
    "# training_scores_y\n",
    "# testing_scores_y\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(training_scores_y)\n",
    "encoded_Y = encoder.transform(training_scores_y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=12, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train_scaled, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train_scaled, encoded_Y, cv=kfold)\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.071Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "itera = 1000\n",
    "r = MLPRegressor(max_iter=itera)\n",
    "r.fit(X_train_scaled, training_scores_y)\n",
    "r.score(X_train_scaled, training_scores_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(training_scores_y, train_prediction)[0]\n",
    "b = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('MLP Training Accuracy: ',round(a,3))\n",
    "print('MLP Testing Accuracy: ', round(b,3))\n",
    "print('MLP Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLP Training Accuracy:',round(a,3), 'w/mod_:', round(c,3), '| SVR Training Accuracy:',round(e,3),'| KNN Training Accuracy:',round(g,3))\n",
    "print('MLP Testing Accuracy :', round(b,3), 'w/mod_:', round(d,3),'| SVR Testing Accuracy :', round(f,3), '| KNN Testing Accuracy :', round(h,3))\n",
    "print('MLP Drop Train-Test  :', round(a-b,3), 'w/mod_:', round(c-d,3),'| SVR Drop Train-Test : ', round(e-f,3), '| KNN Drop Train-Test  :', round(g-h,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be logged to your sklearn-demos project on Comet.ml\n",
    "params={\"main experiment changes\":what_changed,\n",
    "        \"random_state\":random_state,\n",
    "        \"MLP_iterations\": itera,\n",
    "        \"MLP_solver\":solv,\n",
    "        \"MLP_activation\":activ,\n",
    "        \"SVM_gamma\":gamma_type,\n",
    "        \"SVM_C\":c_val,\n",
    "        \"SVM_epsilon\":epsilon_val\n",
    "        #\"stratify\":True\n",
    "}\n",
    "\n",
    "metrics = {'MLP Training Accuracy':a,\n",
    "'MLP Testing Accuracy':b,\n",
    "'MLP Drop Train-Test':a-b,\n",
    "'MLP_mod Training Accuracy':c,\n",
    "'MLP_mod Testing Accuracy':d,\n",
    "'MLP_mod Drop Train-Test':c-d,\n",
    "'SVM Training Accuracy':e,\n",
    "'SVM Testing Accuracy':f,\n",
    "'SVM Drop Train-Test':e-f,\n",
    "'KNN Training Accuracy':g,\n",
    "'KNN Testing Accuracy':h,\n",
    "'KNN Drop Train-Test':g-h\n",
    "}\n",
    "\n",
    "experiment.log_dataset_hash(X_train_scaled)\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
