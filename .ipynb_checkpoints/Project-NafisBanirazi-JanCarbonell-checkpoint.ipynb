{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exercise - Session 3\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this lab are the following: \n",
    "- To read all pairs of sentences from a trial set, compute their similarities considering lemmatization and using the Jaccard distance. \n",
    "- Compare the results with those on session 2.\n",
    "- Compare the results with the gold standard by giving the Pearson correlation.\n",
    "- Answering if words or lemmas perfom better and if we could extrapolate that to any pair of texts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#variable initialization and instantiation\n",
    "d = {}\n",
    "tests = []\n",
    "tests_lem = []\n",
    "standard = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "\n",
    "train_input = text_to_sentences('../00_data/trial/STS.input_fixed.txt')\n",
    "train_input = text_to_sentences('../00_data/train/STS.input.MSRpar.txt')\n",
    "\n",
    "test_input = text_to_sentences('../00_data/test-gold/STS.input.MSRpar.txt')\n",
    "\n",
    "train_classes = open('../00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "#train_classes = open('../00_data/train/STS.gs.MSRpar.txt', encoding=\"utf-8-sig\").readlines()\n",
    "test_classes = open('../00_data/test-gold/STS.gs.MSRpar.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the Jaccard distance. In this step, we must first tokenize the sentences. We then take the **sets**; *unique values of those tokenized sentences*, lemmatize them and compute the **jaccard similarity as 1 - jaccard distance**. \n",
    "\n",
    "We also run the initial jaccard_similarity operation without the lemmatization to be able to compare the results directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.333, 0.412, 0.571, 0.333, 0.167, 0.138]\n"
     ]
    }
   ],
   "source": [
    "for key in d:\n",
    "    sents = [nltk.word_tokenize(s) for s in d[key]]\n",
    "        \n",
    "    w1 = set(sents[0])\n",
    "    w2 = set(sents[1])\n",
    "    \n",
    "    wordtype1 = pos_tag(w1)\n",
    "    lem1 = set([lemmatize(w) for w in wordtype1])\n",
    "    \n",
    "    wordtype2 = pos_tag(w2)\n",
    "    lem2 = set([lemmatize(w) for w in wordtype2])\n",
    "    \n",
    "    jaccard_similarity = 1 - jaccard_distance(w1, w2)\n",
    "    jaccard_similarity_lem = 1 - jaccard_distance(lem1, lem2)\n",
    "    tests.append(round(jaccard_similarity,3))\n",
    "    tests_lem.append(round(jaccard_similarity_lem,3))\n",
    "print(tests_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we open the Golden Standard file and calculate the perason correlation with and without lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation without lemmatization: 0.396\n",
      "Pearson correlation with lemmatization: 0.65\n"
     ]
    }
   ],
   "source": [
    "for line in open('./trial/STS.gs.txt','r'):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    standard.append(int(line[1]))\n",
    "\n",
    "a = pearsonr(standard[0:6], tests[0:6])[0]\n",
    "b = pearsonr(standard[0:6], tests_lem[0:6])[0]\n",
    "print('Pearson correlation without lemmatization:', round(a,3))\n",
    "print('Pearson correlation with lemmatization:', round(b,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Over the development of this lab, we have computed the similarities between different pairs of sentences; calculated their jaccard distance and performed the same operattion with lemmatization in between. \n",
    "\n",
    "We have also compared them with the gold standard and have achieved significantly better results:\n",
    "\n",
    "*Pearson correlation without lemmatization: 0.414*\n",
    "*Pearson correlation with lemmatization: 0.652*\n",
    "\n",
    "**Can this be extrapolated to any pair of texts?**\n",
    "Yes.  The essence of lemmatization is to reduce them to their infinitive forms. The problem with the English language (and most of the others) is that words have several forms with the same semantic meanings. This reduction allows us to extract the core meaning of a word. It is also considered better than other methods such as Stemming -which uses heuristics- as Lemmatization seeks to remove only the inflectional endings. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
