{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Students: Nafis Banirazi & Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "\n",
    "#variable initialization and instantiation\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std_train = []\n",
    "gold_std_test = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    # Clean the text\n",
    "    line = regex.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line)\n",
    "    line = regex.sub(r\"what's\", \"what is \", line)\n",
    "    line = regex.sub(r\"\\'s\", \" \", line)\n",
    "    line = regex.sub(r\"\\'ve\", \" have \", line)\n",
    "    line = regex.sub(r\"can't\", \"cannot \", line)\n",
    "    line = regex.sub(r\"n't\", \" not \", line)\n",
    "    line = regex.sub(r\"i'm\", \"i am \", line)\n",
    "    line = regex.sub(r\"\\'re\", \" are \", line)\n",
    "    line = regex.sub(r\"\\'d\", \" would \", line)\n",
    "    line = regex.sub(r\"\\'ll\", \" will \", line)\n",
    "    line = regex.sub(r\",\", \" \", line)\n",
    "    line = regex.sub(r\"\\.\", \" \", line)\n",
    "    line = regex.sub(r\"!\", \" ! \", line)\n",
    "    line = regex.sub(r\"\\/\", \" \", line)\n",
    "    line = regex.sub(r\"\\^\", \" ^ \", line)\n",
    "    line = regex.sub(r\"\\+\", \" + \", line)\n",
    "    line = regex.sub(r\"\\-\", \" - \", line)\n",
    "    line = regex.sub(r\"\\=\", \" = \", line)\n",
    "    line = regex.sub(r\"'\", \" \", line)\n",
    "    line = regex.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", line)\n",
    "    line = regex.sub(r\":\", \" : \", line)\n",
    "    line = regex.sub(r\" e g \", \" eg \", line)\n",
    "    line = regex.sub(r\" b g \", \" bg \", line)\n",
    "    line = regex.sub(r\" u s \", \" american \", line)\n",
    "    line = regex.sub(r\"\\0s\", \"0\", line)\n",
    "    line = regex.sub(r\" 9 11 \", \"911\", line)\n",
    "    line = regex.sub(r\"e - mail\", \"email\", line)\n",
    "    line = regex.sub(r\"j k\", \"jk\", line)\n",
    "    line = regex.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    #only accept alphanum\n",
    "    # [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    #remove punctuation\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "# convert words to tokens\n",
    "def tokens_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(token_to_lemmas, tokens))\n",
    "\n",
    "def token_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Standarization\n",
    "### Regular Expression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaccard distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Potential Networks\n",
    "- MLPRegressor --> Using regressors\n",
    "- Perceptron\n",
    "- LinealRegression -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(tokens_0), tokens_to_lemmas(tokens_1)\n",
    "    \n",
    "    #jaccard_similarity\n",
    "    #print (float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), '\\n')\n",
    "    return float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Training!\n",
      "\n",
      "Testing\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Testing!\n",
      "\n",
      "Results\n",
      "########################################################################\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training')\n",
    "training_data_X = [lemma_jaccard(data[0], data[1]) for data in train_input]\n",
    "training_scores_y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "\n",
    "print('Testing')\n",
    "testing_data_X = [lemma_jaccard(data[0], data[1])for data in test_input]\n",
    "testing_scores_y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "print('Results')\n",
    "#maybe label encoding https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous\n",
    "#print(training_data_X)\n",
    "#print('########################################################################\\n')\n",
    "#print(np.array(training_data_X).reshape(-1,1))\n",
    "#print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n')\n",
    "#print(training_scores_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pandas DataFrame\n",
    "df_X = pd.DataFrame(training_data_X)\n",
    "df_X.head()\n",
    "\n",
    "df_y = pd.Series(training_scores_y)\n",
    "df_y.head() \n",
    "\n",
    "df_X_Test = pd.DataFrame(testing_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "Training Accuracy:  0.69\n",
      "Testing Accuracy:  0.627\n",
      "Drop Train-Test:  0.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/.local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "r = MLPRegressor()\n",
    "r.fit(df_X, df_y)\n",
    "r.score(df_X, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(df_X).tolist()\n",
    "test_prediction = r.predict(df_X_Test).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(training_scores_y, train_prediction)[0]\n",
    "b = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('Training Accuracy: ',round(a,3))\n",
    "print('Testing Accuracy: ', round(b,3))\n",
    "print('Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
