{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "### Student: Jan Carbonell\n",
    "\n",
    "### Lab Objective:\n",
    "The Objective of this project are the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET ERROR: File could not be uploaded\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/jcllobet/general/bd1e056699354a00acf17fa186b1c396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml in the top of the file for experiment tracking\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"WgXEAqBycAS6nrjJC5zkNTLA2\",\n",
    "                        project_name=\"general\", workspace=\"jcllobet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.778257Z",
     "start_time": "2018-12-11T01:01:53.997302Z"
    }
   },
   "outputs": [],
   "source": [
    "#intial set of imports\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.metrics import jaccard_distance, edit_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "#variable initialization and instantiation\n",
    "tests = []\n",
    "tests_lem = []\n",
    "gold_std_train = []\n",
    "gold_std_test = []\n",
    "lem1 = []\n",
    "lem2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read all pairs of sentences of the train and test set\n",
    "We proceed to open and read the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.800801Z",
     "start_time": "2018-12-11T01:01:54.781241Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "# TRIAL TESTING\n",
    "trial_input = text_to_sentences('./00_data/trial/STS.input_fixed.txt')\n",
    "trial_classes = open('./00_data/trial/STS_fixed.gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TRAINING PHASE\n",
    "train_input = text_to_sentences('./00_data/train/STS.input.MSRpar_vid_SMT.txt')\n",
    "train_classes = open('./00_data/train/STS.gs.MSRpar_vid_SMT.txt', encoding=\"utf-8-sig\").readlines()\n",
    "\n",
    "# TESTING PHASE\n",
    "test_input = text_to_sentences('./00_data/test-gold/STS.input.ALL.txt')\n",
    "test_classes = open('./00_data/test-gold/STS.gs.ALL.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the lematizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Noisy entities removal functions\n",
    "### Stopwords, URL's, Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.816260Z",
     "start_time": "2018-12-11T01:01:54.802577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the punctuation and lowering the case of a string\n",
    "def preprocessing(line):\n",
    "    \n",
    "    line = line.lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    line = regex.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line)\n",
    "    line = regex.sub(r\"what's\", \"what is \", line)\n",
    "    line = regex.sub(r\"\\'s\", \" \", line)\n",
    "    line = regex.sub(r\"\\'ve\", \" have \", line)\n",
    "    line = regex.sub(r\"can't\", \"cannot \", line)\n",
    "    line = regex.sub(r\"n't\", \" not \", line)\n",
    "    line = regex.sub(r\"i'm\", \"i am \", line)\n",
    "    line = regex.sub(r\"\\'re\", \" are \", line)\n",
    "    line = regex.sub(r\"\\'d\", \" would \", line)\n",
    "    line = regex.sub(r\"\\'ll\", \" will \", line)\n",
    "    line = regex.sub(r\",\", \" \", line)\n",
    "    line = regex.sub(r\"\\.\", \" \", line)\n",
    "    line = regex.sub(r\"!\", \" ! \", line)\n",
    "    line = regex.sub(r\"\\/\", \" \", line)\n",
    "    line = regex.sub(r\"\\^\", \" ^ \", line)\n",
    "    line = regex.sub(r\"\\+\", \" + \", line)\n",
    "    line = regex.sub(r\"\\-\", \" - \", line)\n",
    "    line = regex.sub(r\"\\=\", \" = \", line)\n",
    "    line = regex.sub(r\"'\", \" \", line) #careful, it used to be \" \". Testing again for accuracy purposes. \n",
    "    line = regex.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", line)\n",
    "    line = regex.sub(r\":\", \" : \", line)\n",
    "    line = regex.sub(r\" e g \", \" eg \", line)\n",
    "    line = regex.sub(r\" b g \", \" bg \", line)\n",
    "    line = regex.sub(r\" u s \", \" american \", line)\n",
    "    line = regex.sub(r\"\\0s\", \"0\", line)\n",
    "    line = regex.sub(r\" 9 11 \", \"911\", line)\n",
    "    line = regex.sub(r\"e - mail\", \"email\", line)\n",
    "    line = regex.sub(r\"j k\", \"jk\", line)\n",
    "    line = regex.sub(r\"\\s{2,}\", \" \", line)\n",
    "\n",
    "    #only accept alphanum\n",
    "    # [w for w in doc if w.isalpha()]  # Remove numbers and punctuation.\n",
    "    #remove punctuation\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word normalization\n",
    "### Tokenization, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T01:01:54.830135Z",
     "start_time": "2018-12-11T01:01:54.819191Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the words from the sentence minus stopwords\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def stopwords_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "# convert words to tokens\n",
    "def pos_tag_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(pos_tag_to_lemmas, tokens))\n",
    "\n",
    "def pos_tag_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synset, Nammed Entity and Content Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.020Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesking_sentence(pos_tagged_sentence):\n",
    "    \"\"\"\n",
    "    Returns a sentence as the given sentece using lesker algorithms.\n",
    "    The input sentence must be a pos_tagged sentence (e.g. [('The', 'DN'),\n",
    "    ('sun', 'NN')]).\n",
    "    \"\"\"\n",
    "    sentence = [i[0] for i in pos_tagged_sentence]\n",
    "    result = []\n",
    "    \n",
    "    none_type_objects = []\n",
    "    for word, tag in pos_tagged_sentence:\n",
    "        # 'NoneType' object has no attribute 'name'\n",
    "        try:\n",
    "            result.append(lesk(sentence,word, wordnet_pos_code(tag)).name())\n",
    "        except:\n",
    "            result.append(word)            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Potential Networks\n",
    "- MLPRegressor --> Using regressors\n",
    "- Support Vector Regressor\n",
    "- KNN Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.026Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.030Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemma(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    \n",
    "    #jaccard_similarity & edit similarity\n",
    "    #print ('jaccard:', float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), 'edit_dist:', float(edit_distance(lemmas_0, lemmas_1)))\n",
    "    return [float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))),float(edit_distance(lemmas_0, lemmas_1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.034Z"
    }
   },
   "outputs": [],
   "source": [
    "def lesk_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lesk_0, lesk_1 = lesking_sentence(pos_0), lesking_sentence(pos_1)\n",
    "    \n",
    "    #jaccard_similarity\n",
    "    #print (float(1 - jaccard_distance(set(lesk_0), set(lesk_1))), '\\n')\n",
    "    return float(1 - jaccard_distance(set(lesk_0), set(lesk_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.050Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_feature_generation(sent_0, send_1):\n",
    "    featureset = {}\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = preprocessing(sent_0), preprocessing(sent_1)\n",
    "    #print(sent_0 + '\\n' + sent_1 + '\\n')\n",
    "    token_0, token_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    pos_0, pos_1 = pos_tag_from_words(token_0), pos_tag_from_words(token_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(pos_0), tokens_to_lemmas(pos_1)\n",
    "    \n",
    "    #jaccard_similarity & edit similarity\n",
    "    print ('jaccard:', float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))), 'edit_dist:', float(edit_distance(lemmas_0, lemmas_1)))\n",
    "    return [float(1 - jaccard_distance(set(lemmas_0), set(lemmas_1))),float(edit_distance(lemmas_0, lemmas_1))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.055Z"
    }
   },
   "outputs": [],
   "source": [
    "#print('Training Lemmas')\n",
    "#training_data_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in train_input]\n",
    "#training_data_X_lemma_edit = [lemma(data[0], data[1])[1] for data in train_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data with Lemmas\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Training data with Lesk\n",
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Training!\n",
      "\n",
      "Testing data with Lemmas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Testing Lesk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Finished Testing!\n",
      "\n",
      "Results\n"
     ]
    }
   ],
   "source": [
    "print('Training data with Lemmas')\n",
    "trn_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in train_input]\n",
    "trn_X_lemma_edit = [lemma(data[0], data[1])[1] for data in train_input]\n",
    "\n",
    "print('Training data with Lesk')\n",
    "trn_X_lesk = [lesk_jaccard(data[0], data[1]) for data in train_input]\n",
    "training_scores_y = [float(line.strip()) for line in train_classes]\n",
    "print('Finished Training!\\n')\n",
    "\n",
    "print('Testing data with Lemmas')\n",
    "tst_X_lemma_jaccard = [lemma(data[0], data[1])[0] for data in test_input]\n",
    "tst_X_lemma_edit = [lemma(data[0], data[1])[1] for data in test_input]\n",
    "\n",
    "print('Testing Lesk')\n",
    "tst_X_lesk = [lesk_jaccard(data[0], data[1])for data in test_input]\n",
    "\n",
    "testing_scores_y = [float(line.strip()) for line in test_classes]\n",
    "print('Finished Testing!\\n')\n",
    "\n",
    "print('Results')\n",
    "#maybe label encoding https://www.kaggle.com/pratsiuk/valueerror-unknown-label-type-continuous\n",
    "#print(training_data_X)\n",
    "#print('########################################################################\\n')\n",
    "#print(np.array(training_data_X).reshape(-1,1))\n",
    "#print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\n')\n",
    "#print(training_scores_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>trainning lemma edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.357143</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  trainning lemma edit\n",
       "0  0.473684                   7.0\n",
       "1  0.500000                   5.0\n",
       "2  0.357143                   7.0\n",
       "3  0.611111                   6.0\n",
       "4  0.111111                  14.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Creating a Pandas DataFrame\n",
    "df_X_train = pd.DataFrame(trn_X_lemma_jaccard)\n",
    "df_X_train[\"trainning lemma edit\"] = pd.DataFrame(trn_X_lemma_edit)\n",
    "#df_X = pd.DataFrame(training_data_X_lesk)\n",
    "#df_X[\"trainning lesk\"] = pd.DataFrame(training_data_X_lesk)\n",
    "\n",
    "df_y = pd.Series(training_scores_y)\n",
    "\n",
    "df_X_test = pd.DataFrame(tst_X_lemma_jaccard)\n",
    "#df_X_Test = pd.DataFrame(testing_data_X_lesk)\n",
    "#df_X_Test[\"testing lesk\"] = pd.DataFrame(testing_data_X_lesk)\n",
    "df_X_test[\"testing lemma edit\"] = pd.DataFrame(tst_X_lemma_edit)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(df_X_train)\n",
    "X_test_scaled = scaler.transform(df_X_test)\n",
    "\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_changed = \"Normalized the featureset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n",
      "MLP Training Accuracy:  0.77\n",
      "MLP Testing Accuracy:  0.662\n",
      "MLP Drop Train-Test:  0.108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "itera = 1000\n",
    "r = MLPRegressor(max_iter=itera)\n",
    "r.fit(X_train_scaled, df_y)\n",
    "r.score(X_train_scaled, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "a = pearsonr(training_scores_y, train_prediction)[0]\n",
    "b = pearsonr(testing_scores_y, test_prediction)[0]\n",
    "print('MLP Training Accuracy: ',round(a,3))\n",
    "print('MLP Testing Accuracy: ', round(b,3))\n",
    "print('MLP Drop Train-Test: ', round(a-b,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pytorch instead?\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#activation tanh or logistic give better performance\n",
    "solv = 'lbfgs'\n",
    "activ = 'logistic'\n",
    "r = MLPRegressor(max_iter=itera, solver=solv, hidden_layer_sizes=(100,50), activation=activ)\n",
    "r.fit(X_train_scaled, df_y)\n",
    "r.score(X_train_scaled, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "c = pearsonr(training_scores_y, train_prediction)[0]\n",
    "d = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "gamma_type = 'scale'\n",
    "c_val = 1.0\n",
    "epsilon_val = 0.2\n",
    "r = SVR(gamma=gamma_type, C=c_val, epsilon=epsilon_val)\n",
    "r.fit(X_train_scaled, df_y)\n",
    "r.score(X_train_scaled, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "e = pearsonr(training_scores_y, train_prediction)[0]\n",
    "f = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-11T01:01:54.086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are ready!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "r = KNeighborsRegressor(n_neighbors=15)\n",
    "r.fit(X_train_scaled, df_y)\n",
    "r.score(X_train_scaled, df_y)\n",
    "\n",
    "# do the prediction with train --> to evaluate where the model could improve and TRAIN --> To get actual results\n",
    "train_prediction = r.predict(X_train_scaled).tolist()\n",
    "test_prediction = r.predict(X_test_scaled).tolist()\n",
    "\n",
    "# Evaluation of the prediction\n",
    "print('Results are ready!\\n')\n",
    "g = pearsonr(training_scores_y, train_prediction)[0]\n",
    "h = pearsonr(testing_scores_y, test_prediction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Training Accuracy: 0.77 w/mod_: 0.805 | SVR Training Accuracy: 0.772 | KNN Training Accuracy: 0.785\n",
      "MLP Testing Accuracy : 0.662 w/mod_: 0.626 | SVR Testing Accuracy : 0.654 | KNN Testing Accuracy : 0.655\n",
      "MLP Drop Train-Test  : 0.108 w/mod_: 0.179 | SVR Drop Train-Test :  0.118 | KNN Drop Train-Test  : 0.131\n"
     ]
    }
   ],
   "source": [
    "print('MLP Training Accuracy:',round(a,3), 'w/mod_:', round(c,3), '| SVR Training Accuracy:',round(e,3),'| KNN Training Accuracy:',round(g,3))\n",
    "print('MLP Testing Accuracy :', round(b,3), 'w/mod_:', round(d,3),'| SVR Testing Accuracy :', round(f,3), '| KNN Testing Accuracy :', round(h,3))\n",
    "print('MLP Drop Train-Test  :', round(a-b,3), 'w/mod_:', round(c-d,3),'| SVR Drop Train-Test : ', round(e-f,3), '| KNN Drop Train-Test  :', round(g-h,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these will be logged to your sklearn-demos project on Comet.ml\n",
    "params={\"main experiment changes\":what_changed,\n",
    "        \"random_state\":random_state,\n",
    "        \"MLP_iterations\": itera,\n",
    "        \"MLP_solver\":solv,\n",
    "        \"MLP_activation\":activ,\n",
    "        \"SVM_gamma\":gamma_type,\n",
    "        \"SVM_C\":c_val,\n",
    "        \"SVM_epsilon\":epsilon_val\n",
    "        #\"stratify\":True\n",
    "}\n",
    "\n",
    "metrics = {'MLP Training Accuracy':a,\n",
    "'MLP Testing Accuracy':b,\n",
    "'MLP Drop Train-Test':a-b,\n",
    "'MLP_mod Training Accuracy':c,\n",
    "'MLP_mod Testing Accuracy':d,\n",
    "'MLP_mod Drop Train-Test':c-d,\n",
    "'SVM Training Accuracy':e,\n",
    "'SVM Testing Accuracy':f,\n",
    "'SVM Drop Train-Test':e-f,\n",
    "'KNN Training Accuracy':g,\n",
    "'KNN Testing Accuracy':h,\n",
    "'KNN Drop Train-Test':g-h\n",
    "}\n",
    "\n",
    "experiment.log_dataset_hash(X_train_scaled)\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
